[
  {
    "objectID": "about/Sue-EllenDuffy.html",
    "href": "about/Sue-EllenDuffy.html",
    "title": "Sue-Ellen Duffy",
    "section": "",
    "text": "UMass Amherst 2009-2013 Public Health - Community Health and Education\nUMass Amherst 2023-present DACSS\nFreelance Education Research (Observer, Assessor) - Abt Associates, Boston, MA\nAcademic Services Researcher - Jewish Vocational Services, Boston, MA\nFreelance Education Researcher (Assessor) - Harvard University, Boston, MA\nNonprofit Research and Food Justice Work - Fair Foods, Boston, MA\nFacilitation Trainer - UMass Amherst, MA"
  },
  {
    "objectID": "about/Sue-EllenDuffy.html#r-experience",
    "href": "about/Sue-EllenDuffy.html#r-experience",
    "title": "Sue-Ellen Duffy",
    "section": "R experience",
    "text": "R experience\nLimited - DACSS 601 is my first exposure!"
  },
  {
    "objectID": "about/Sue-EllenDuffy.html#research-interests",
    "href": "about/Sue-EllenDuffy.html#research-interests",
    "title": "Sue-Ellen Duffy",
    "section": "Research interests",
    "text": "Research interests\nEducation, Gender, Food Justice/Access, Connection, PAR"
  },
  {
    "objectID": "about/Sue-EllenDuffy.html#hometown",
    "href": "about/Sue-EllenDuffy.html#hometown",
    "title": "Sue-Ellen Duffy",
    "section": "Hometown",
    "text": "Hometown\nI grew up in The South Shore of Massachusetts and have lived in Boston now for 7 years."
  },
  {
    "objectID": "about/Sue-EllenDuffy.html#hobbies",
    "href": "about/Sue-EllenDuffy.html#hobbies",
    "title": "Sue-Ellen Duffy",
    "section": "Hobbies",
    "text": "Hobbies\nForaging, Gardening, Sunbasking, Bar Pizza"
  },
  {
    "objectID": "about/Sue-EllenDuffy.html#fun-fact",
    "href": "about/Sue-EllenDuffy.html#fun-fact",
    "title": "Sue-Ellen Duffy",
    "section": "Fun fact",
    "text": "Fun fact\nI once went on tour with the band They Might Be Giants - I was their truck driver and assistant manager ;)"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Contributors",
    "section": "",
    "text": "Find out more about our DACSS students who contributed to the blog.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSue-Ellen Duffy\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DACSS 601: Data Science Fundamentals Spring-2023",
    "section": "",
    "text": "The blog posts here are contributed by students enrolled in DACSS 601, Fundamentals of Data Science. The course provides students with an introduction to R and the tidyverse, scientific publishing, and collaboration through GitHub, building a foundation for future coursework. Students also are introduced to general data management and data wrangling skills, with an emphasis on best practice workflows and tidy data management.\n\n\n\n\n\n\n\n\n\n\nAirBnB New York 2019 Challenge 5\n\n\n\n\n\n\n\nchallenge_5\n\n\nSue-Ellen Duffy\n\n\nairbnb\n\n\n\n\nPlotting Price per Neighbourhood Groups\n\n\n\n\n\n\nMar 25, 2023\n\n\nSue-Ellen Duffy\n\n\n\n\n\n\n\n\nChallenge 4 Egg Data\n\n\n\n\n\n\n\nchallenge_4\n\n\nSue-Ellen Duffy\n\n\neggs\n\n\n\n\nData wrangling: Mutate\n\n\n\n\n\n\nMar 22, 2023\n\n\nSue-Ellen Duffy\n\n\n\n\n\n\n\n\nChallenge 3: Eggs 2004-2013\n\n\n\n\n\n\n\nchallenge_3\n\n\neggs\n\n\nSue-Ellen Duffy\n\n\n\n\nPivoting Egg Data\n\n\n\n\n\n\nMar 14, 2023\n\n\nSue-Ellen Duffy\n\n\n\n\n\n\n\n\nUnderstanding the FAOSTAT Country Codes\n\n\n\n\n\n\n\nchallenge_2\n\n\nSue-Ellen Duffy\n\n\nFAOSTAT\n\n\n\n\n\n\n\n\n\n\n\nFeb 24, 2023\n\n\nSue-Ellen Duffy\n\n\n\n\n\n\n\n\nRailroad Employees Challenge 1\n\n\n\n\n\n\n\nchallenge_1\n\n\nSue-Ellen Duffy\n\n\nRailroad Employee Dataset\n\n\n\n\n\n\n\n\n\n\n\nFeb 23, 2023\n\n\nSue-Ellen Duffy\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/SDuffy_challenge_1.html",
    "href": "posts/SDuffy_challenge_1.html",
    "title": "Railroad Employees Challenge 1",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(summarytools)\n\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/SDuffy_challenge_1.html#reading-in-the-data",
    "href": "posts/SDuffy_challenge_1.html#reading-in-the-data",
    "title": "Railroad Employees Challenge 1",
    "section": "Reading in the Data",
    "text": "Reading in the Data\nI analyzed the “railroad_2012_county_clean.csv” data for Challenge 1. This data describes the Total Number of Railroad Employees by County and State in the United States in 2012. Upon first glance the data contains 3 columns and 2,930 rows. The columns are: state, county, and total_employees\n\n\nCode\n#Read in data and rename railroad_2012_clean_county as data\ndata <- rename(read_csv(\"_data/railroad_2012_clean_county.csv\"))\n\n\nRows: 2930 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): state, county\ndbl (1): total_employees\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\n#Preview data \ndata\n\n\n# A tibble: 2,930 × 3\n   state county               total_employees\n   <chr> <chr>                          <dbl>\n 1 AE    APO                                2\n 2 AK    ANCHORAGE                          7\n 3 AK    FAIRBANKS NORTH STAR               2\n 4 AK    JUNEAU                             3\n 5 AK    MATANUSKA-SUSITNA                  2\n 6 AK    SITKA                              1\n 7 AK    SKAGWAY MUNICIPALITY              88\n 8 AL    AUTAUGA                          102\n 9 AL    BALDWIN                          143\n10 AL    BARBOUR                            1\n# … with 2,920 more rows"
  },
  {
    "objectID": "posts/SDuffy_challenge_1.html#summary-of-data",
    "href": "posts/SDuffy_challenge_1.html#summary-of-data",
    "title": "Railroad Employees Challenge 1",
    "section": "Summary of Data",
    "text": "Summary of Data\nRunning the dfsummary(data) function shows us:\n\nThe data is complete: there are no missing data.\nTop ten states ranked with the most counties. Texas has the most counties of any other state, accounting for 7.5% of all counties in the United States.\nThere are multiples of county names. We see in the following graph the top 10 county names that are used in the United States. (There are 31 Washington county names in this data plot, that’s far more than I thought there were in the United States!)\n\n\n\nCode\ndfSummary(data)\n\n\nData Frame Summary  \ndata  \nDimensions: 2930 x 3  \nDuplicates: 0  \n\n-----------------------------------------------------------------------------------------------------------------\nNo   Variable          Stats / Values             Freqs (% of Valid)    Graph                Valid      Missing  \n---- ----------------- -------------------------- --------------------- -------------------- ---------- ---------\n1    state             1. TX                       221 ( 7.5%)          I                    2930       0        \n     [character]       2. GA                       152 ( 5.2%)          I                    (100.0%)   (0.0%)   \n                       3. KY                       119 ( 4.1%)                                                   \n                       4. MO                       115 ( 3.9%)                                                   \n                       5. IL                       103 ( 3.5%)                                                   \n                       6. IA                        99 ( 3.4%)                                                   \n                       7. KS                        95 ( 3.2%)                                                   \n                       8. NC                        94 ( 3.2%)                                                   \n                       9. IN                        92 ( 3.1%)                                                   \n                       10. VA                       92 ( 3.1%)                                                   \n                       [ 43 others ]              1748 (59.7%)          IIIIIIIIIII                              \n\n2    county            1. WASHINGTON                31 ( 1.1%)                               2930       0        \n     [character]       2. JEFFERSON                 26 ( 0.9%)                               (100.0%)   (0.0%)   \n                       3. FRANKLIN                  24 ( 0.8%)                                                   \n                       4. LINCOLN                   24 ( 0.8%)                                                   \n                       5. JACKSON                   22 ( 0.8%)                                                   \n                       6. MADISON                   19 ( 0.6%)                                                   \n                       7. MONTGOMERY                18 ( 0.6%)                                                   \n                       8. CLAY                      17 ( 0.6%)                                                   \n                       9. MARION                    17 ( 0.6%)                                                   \n                       10. MONROE                   17 ( 0.6%)                                                   \n                       [ 1699 others ]            2715 (92.7%)          IIIIIIIIIIIIIIIIII                       \n\n3    total_employees   Mean (sd) : 87.2 (283.6)   404 distinct values   :                    2930       0        \n     [numeric]         min < med < max:                                 :                    (100.0%)   (0.0%)   \n                       1 < 21 < 8207                                    :                                        \n                       IQR (CV) : 58 (3.3)                              :                                        \n                                                                        :                                        \n-----------------------------------------------------------------------------------------------------------------\n\n\n\n\nCode\n#How many states are represented in the data?\ndata %>%\n  select(state) %>%\n  n_distinct(.)\n\n\n[1] 53\n\n\nThere are only 50 recognized states, so we need to dig a little deeper to find out what the three additional ‘states’ represent.\n\n\nCode\n#Show unique state data\nunique(data$state)\n\n\n [1] \"AE\" \"AK\" \"AL\" \"AP\" \"AR\" \"AZ\" \"CA\" \"CO\" \"CT\" \"DC\" \"DE\" \"FL\" \"GA\" \"HI\" \"IA\"\n[16] \"ID\" \"IL\" \"IN\" \"KS\" \"KY\" \"LA\" \"MA\" \"MD\" \"ME\" \"MI\" \"MN\" \"MO\" \"MS\" \"MT\" \"NC\"\n[31] \"ND\" \"NE\" \"NH\" \"NJ\" \"NM\" \"NV\" \"NY\" \"OH\" \"OK\" \"OR\" \"PA\" \"RI\" \"SC\" \"SD\" \"TN\"\n[46] \"TX\" \"UT\" \"VA\" \"VT\" \"WA\" \"WI\" \"WV\" \"WY\"\n\n\nAE, AP, and DC are the three non-states cases. AE and AP are military addresses. DC is Washington DC."
  },
  {
    "objectID": "posts/Sue-EllenDuffy_Challenge_2.html",
    "href": "posts/Sue-EllenDuffy_Challenge_2.html",
    "title": "Understanding the FAOSTAT Country Codes",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(summarytools)\nlibrary(readxl)\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/Sue-EllenDuffy_Challenge_2.html#faostat-data",
    "href": "posts/Sue-EllenDuffy_Challenge_2.html#faostat-data",
    "title": "Understanding the FAOSTAT Country Codes",
    "section": "FAOSTAT data",
    "text": "FAOSTAT data\nThis data set is simply the Country Profiles for the Food and Agriculture Organization Corporate Statistical Database (FAOSTAT).\nIn this set of data, each column is describing the same data, but vary in who is describing or using that data. The United Nations Terminology Database, the Statistics Division of the United Nations Secretariat, and the International Organization for Standardization each have different ways of coding the same countries, so this database helps us understand which country or region or group of countries is being described in other FAO data sets.\n\n\nCode\n#Read in data and rename FAOSTAT_country_groups as groups\ndata <- read_csv(\"_data/FAOSTAT_country_groups.csv\")\n\n\nRows: 1943 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): Country Group, Country, M49 Code, ISO2 Code, ISO3 Code\ndbl (2): Country Group Code, Country Code\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\ndata\n\n\n# A tibble: 1,943 × 7\n   `Country Group Code` `Country Group` Countr…¹ Country M49 C…² ISO2 …³ ISO3 …⁴\n                  <dbl> <chr>              <dbl> <chr>   <chr>   <chr>   <chr>  \n 1                 5100 Africa                 4 Algeria 012     DZ      DZA    \n 2                 5100 Africa                 7 Angola  024     AO      AGO    \n 3                 5100 Africa                53 Benin   204     BJ      BEN    \n 4                 5100 Africa                20 Botswa… 072     BW      BWA    \n 5                 5100 Africa               233 Burkin… 854     BF      BFA    \n 6                 5100 Africa                29 Burundi 108     BI      BDI    \n 7                 5100 Africa                35 Cabo V… 132     CV      CPV    \n 8                 5100 Africa                32 Camero… 120     CM      CMR    \n 9                 5100 Africa                37 Centra… 140     CF      CAF    \n10                 5100 Africa                39 Chad    148     TD      TCD    \n# … with 1,933 more rows, and abbreviated variable names ¹​`Country Code`,\n#   ²​`M49 Code`, ³​`ISO2 Code`, ⁴​`ISO3 Code`"
  },
  {
    "objectID": "posts/Sue-EllenDuffy_Challenge_2.html#summarize-the-data",
    "href": "posts/Sue-EllenDuffy_Challenge_2.html#summarize-the-data",
    "title": "Understanding the FAOSTAT Country Codes",
    "section": "Summarize the data",
    "text": "Summarize the data\nLooking through this quick summary we see that there are 277 Countries in this data, that IS02 is missing data, that the M49 Code column is characterized as being characters (though they are all numeric) and while there are other data here, there are two charts of interests - Country Group and Country.\nIS02 is “missing data” because of the code NA which is their 2-alpha code for Nambia.\nM49 Code column is characterized as being characters even though they are all numeric.\nThe Country Group tibble shows us the 10 Country Groups containing the most Countries.\nThe Country tibble shows us the 10 Countries that were categorized into different Country Groups the most.\n\n\nCode\ndfSummary(data)\n\n\nData Frame Summary  \ndata  \nDimensions: 1943 x 7  \nDuplicates: 0  \n\n--------------------------------------------------------------------------------------------------------------------------\nNo   Variable             Stats / Values                  Freqs (% of Valid)    Graph                 Valid      Missing  \n---- -------------------- ------------------------------- --------------------- --------------------- ---------- ---------\n1    Country Group Code   Mean (sd) : 5501.8 (1356.5)     58 distinct values              :           1943       0        \n     [numeric]            min < med < max:                                                : :         (100.0%)   (0.0%)   \n                          336 < 5403 < 9011                                               : :                             \n                          IQR (CV) : 648 (0.2)                                            : :                             \n                                                                                .         : :     .                       \n\n2    Country Group        1. World                        277 (14.3%)           II                    1943       0        \n     [character]          2. Non-Annex I countries        161 ( 8.3%)           I                     (100.0%)   (0.0%)   \n                          3. Net Food Importing Develo     81 ( 4.2%)                                                     \n                          4. Annex I countries             78 ( 4.0%)                                                     \n                          5. High-income economies         64 ( 3.3%)                                                     \n                          6. Africa                        63 ( 3.2%)                                                     \n                          7. Europe                        63 ( 3.2%)                                                     \n                          8. Americas                      61 ( 3.1%)                                                     \n                          9. Small Island Developing S     58 ( 3.0%)                                                     \n                          10. Upper-middle-income econo    56 ( 2.9%)                                                     \n                          [ 48 others ]                   981 (50.5%)           IIIIIIIIII                                \n\n3    Country Code         Mean (sd) : 142.2 (96.4)        277 distinct values   : : : :               1943       0        \n     [numeric]            min < med < max:                                      : : : :               (100.0%)   (0.0%)   \n                          1 < 136 < 622                                         : : : :                                   \n                          IQR (CV) : 133 (0.7)                                  : : : : .                                 \n                                                                                : : : : :         .                       \n\n4    Country              1. Afghanistan                    11 ( 0.6%)                                1943       0        \n     [character]          2. Burkina Faso                   11 ( 0.6%)                                (100.0%)   (0.0%)   \n                          3. Burundi                        11 ( 0.6%)                                                    \n                          4. Central African Republic       11 ( 0.6%)                                                    \n                          5. Chad                           11 ( 0.6%)                                                    \n                          6. Comoros                        11 ( 0.6%)                                                    \n                          7. Ethiopia                       11 ( 0.6%)                                                    \n                          8. Guinea-Bissau                  11 ( 0.6%)                                                    \n                          9. Lesotho                        11 ( 0.6%)                                                    \n                          10. Malawi                        11 ( 0.6%)                                                    \n                          [ 267 others ]                  1833 (94.3%)          IIIIIIIIIIIIIIIIII                        \n\n5    M49 Code             1. 004                            11 ( 0.6%)                                1943       0        \n     [character]          2. 108                            11 ( 0.6%)                                (100.0%)   (0.0%)   \n                          3. 140                            11 ( 0.6%)                                                    \n                          4. 148                            11 ( 0.6%)                                                    \n                          5. 174                            11 ( 0.6%)                                                    \n                          6. 231                            11 ( 0.6%)                                                    \n                          7. 426                            11 ( 0.6%)                                                    \n                          8. 454                            11 ( 0.6%)                                                    \n                          9. 466                            11 ( 0.6%)                                                    \n                          10. 496                           11 ( 0.6%)                                                    \n                          [ 267 others ]                  1833 (94.3%)          IIIIIIIIIIIIIIIIII                        \n\n6    ISO2 Code            1. AF                             11 ( 0.6%)                                1935       8        \n     [character]          2. BF                             11 ( 0.6%)                                (99.6%)    (0.4%)   \n                          3. BI                             11 ( 0.6%)                                                    \n                          4. CF                             11 ( 0.6%)                                                    \n                          5. ET                             11 ( 0.6%)                                                    \n                          6. GW                             11 ( 0.6%)                                                    \n                          7. KM                             11 ( 0.6%)                                                    \n                          8. LS                             11 ( 0.6%)                                                    \n                          9. ML                             11 ( 0.6%)                                                    \n                          10. MN                            11 ( 0.6%)                                                    \n                          [ 266 others ]                  1825 (94.3%)          IIIIIIIIIIIIIIIIII                        \n\n7    ISO3 Code            1. AFG                            11 ( 0.6%)                                1943       0        \n     [character]          2. BDI                            11 ( 0.6%)                                (100.0%)   (0.0%)   \n                          3. BFA                            11 ( 0.6%)                                                    \n                          4. CAF                            11 ( 0.6%)                                                    \n                          5. COM                            11 ( 0.6%)                                                    \n                          6. ETH                            11 ( 0.6%)                                                    \n                          7. GNB                            11 ( 0.6%)                                                    \n                          8. LSO                            11 ( 0.6%)                                                    \n                          9. MLI                            11 ( 0.6%)                                                    \n                          10. MNG                           11 ( 0.6%)                                                    \n                          [ 267 others ]                  1833 (94.3%)          IIIIIIIIIIIIIIIIII                        \n--------------------------------------------------------------------------------------------------------------------------"
  },
  {
    "objectID": "posts/Sue-EllenDuffy_Challenge_2.html#how-are-the-codes-different",
    "href": "posts/Sue-EllenDuffy_Challenge_2.html#how-are-the-codes-different",
    "title": "Understanding the FAOSTAT Country Codes",
    "section": "How are the codes different?",
    "text": "How are the codes different?\nTake the United States of America for example. When filtering for “United States of America” we come out with 8 different rows of data. The Country Code, M49 Code, IS02 Code and IS03 Codes while unique to their specifics are unchanged for the 8 rows. The difference here, and why we get 8 different rows for the United States of America, is that their Country Group Code and Country Group are different. The Country Group Code is simply the number associated with the Country Group. It appears that Country Group is a categorical code, listing the USA as being part of the Americas, High-income economies, North and Central America, Annex I countries, etc.\nThe Country Group would allow for quick and categorical data analysis, such as analyzing the countries by economics (high-income economies and low-income economies) or by region (Northern and Central America to the Americas.\n\n\nCode\nUSA <- filter(data, `Country` == \"United States of America\")\nUSA\n\n\n# A tibble: 8 × 7\n  `Country Group Code` `Country Group`   Count…¹ Country M49 C…² ISO2 …³ ISO3 …⁴\n                 <dbl> <chr>               <dbl> <chr>   <chr>   <chr>   <chr>  \n1                 5200 Americas              231 United… 840     US      USA    \n2                 5848 Annex I countries     231 United… 840     US      USA    \n3                 9010 High-income econ…     231 United… 840     US      USA    \n4                  336 North and Centra…     231 United… 840     US      USA    \n5                 5203 Northern America      231 United… 840     US      USA    \n6                 5208 Northern America…     231 United… 840     US      USA    \n7                 5873 OECD                  231 United… 840     US      USA    \n8                 5000 World                 231 United… 840     US      USA    \n# … with abbreviated variable names ¹​`Country Code`, ²​`M49 Code`, ³​`ISO2 Code`,\n#   ⁴​`ISO3 Code`"
  },
  {
    "objectID": "posts/Sue-EllenDuffy_Challenge_2.html#the-world",
    "href": "posts/Sue-EllenDuffy_Challenge_2.html#the-world",
    "title": "Understanding the FAOSTAT Country Codes",
    "section": "The “World”",
    "text": "The “World”\nThe one Country Group that contains all of the countries is “World”, which consists of 277 Countries. If the goal was to analyze all the countries at once, the filter should be set to “World”.\n\n\nCode\nworld <- filter(data, `Country Group` == \"World\")\nworld\n\n\n# A tibble: 277 × 7\n   `Country Group Code` `Country Group` Countr…¹ Country M49 C…² ISO2 …³ ISO3 …⁴\n                  <dbl> <chr>              <dbl> <chr>   <chr>   <chr>   <chr>  \n 1                 5000 World                  2 Afghan… 004     AF      AFG    \n 2                 5000 World                284 Åland … 248     F284    ALA    \n 3                 5000 World                  3 Albania 008     AL      ALB    \n 4                 5000 World                  4 Algeria 012     DZ      DZA    \n 5                 5000 World                  5 Americ… 016     AS      ASM    \n 6                 5000 World                  6 Andorra 020     AD      AND    \n 7                 5000 World                  7 Angola  024     AO      AGO    \n 8                 5000 World                258 Anguil… 660     AI      AIA    \n 9                 5000 World                 30 Antarc… 010     AQ      ATA    \n10                 5000 World                  8 Antigu… 028     AG      ATG    \n# … with 267 more rows, and abbreviated variable names ¹​`Country Code`,\n#   ²​`M49 Code`, ³​`ISO2 Code`, ⁴​`ISO3 Code`"
  },
  {
    "objectID": "posts/Sue-EllenDuffy_Challenge_2.html#anything-else-interesting",
    "href": "posts/Sue-EllenDuffy_Challenge_2.html#anything-else-interesting",
    "title": "Understanding the FAOSTAT Country Codes",
    "section": "Anything Else Interesting?",
    "text": "Anything Else Interesting?\nThe M49 Codes with the suffix “.01” are characterized as being “unspecified (population)”. I’m not entirely sure what that means, so it could be interesting to understand this further. Here is one example:\n\n\nCode\ndata %>%\n  filter(`M49 Code` == \"155.01\") %>%\nselect(\"Country Group\", \"Country\", \"M49 Code\")\n\n\n# A tibble: 3 × 3\n  `Country Group` Country                                  `M49 Code`\n  <chr>           <chr>                                    <chr>     \n1 Europe          Western Europe, unspecified (population) 155.01    \n2 Western Europe  Western Europe, unspecified (population) 155.01    \n3 World           Western Europe, unspecified (population) 155.01"
  },
  {
    "objectID": "posts/Sue-EllenDuffy_Challenge_3.html",
    "href": "posts/Sue-EllenDuffy_Challenge_3.html",
    "title": "Challenge 3: Eggs 2004-2013",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(summarytools)\nlibrary(readxl)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/Sue-EllenDuffy_Challenge_3.html#reading-in-the-egg-data",
    "href": "posts/Sue-EllenDuffy_Challenge_3.html#reading-in-the-egg-data",
    "title": "Challenge 3: Eggs 2004-2013",
    "section": "Reading in the egg data",
    "text": "Reading in the egg data\n\n\nCode\neggs_og <- read_excel(\"_data/organiceggpoultry.xls\",\n                      sheet=\"Data\",\n                      range =cell_limits(c(6,2),c(NA,6)),\n                      col_names = c(\"date\", \"xlarge_dzn\", \"xlarge_halfdzn\", \"large_dzn\", \"large_halfdzn\")\n)\neggs_og\n\n\n# A tibble: 120 × 5\n   date      xlarge_dzn xlarge_halfdzn large_dzn large_halfdzn\n   <chr>          <dbl>          <dbl>     <dbl>         <dbl>\n 1 Jan 2004        230            132       230           126 \n 2 February        230            134.      226.          128.\n 3 March           230            137       225           131 \n 4 April           234.           137       225           131 \n 5 May             236            137       225           131 \n 6 June            241            137       231.          134.\n 7 July            241            137       234.          134.\n 8 August          241            137       234.          134.\n 9 September       241            136.      234.          130.\n10 October         241            136.      234.          128.\n# … with 110 more rows\n\n\n\n\n\n\n\n\nDate Format 1\n\n\n\nStarting off a little messy. Already I see this data is -wide- and the date needs formatting. Let’s see what other nuances might be lingering in the date column (count).\n\n\n\n\nCode\ntable(select(eggs_og, date))\n\n\ndate\n      April      August    December    February February /1    Jan 2004 \n         10          10          10           8           2           1 \n   Jan 2005    Jan 2006    Jan 2007    Jan 2008    Jan 2009    Jan 2010 \n          1           1           1           1           1           1 \n   Jan 2011    Jan 2012    Jan 2013        July        June       March \n          1           1           1          10          10          10 \n        May    November     October   September \n         10          10          10          10 \n\n\n\n\n\n\n\n\nDate Format 2\n\n\n\nIn the date column, January has a year indicator, 10 of the months appear 10 times, February appears 8 times and February/1 (a leap year) appears twice. We have to delete the /1 in February (mutate) and extend the year indicator from January to the rest of the months (separate and fill).\n\n\n\n\nCode\neggs <- eggs_og %>%\n  mutate(date = str_remove(date, \" /1\")) %>%\n  separate(date,into=c(\"month\", \"year\"), sep=\" \") %>%\n  fill(year)\neggs\n\n\n# A tibble: 120 × 6\n   month     year  xlarge_dzn xlarge_halfdzn large_dzn large_halfdzn\n   <chr>     <chr>      <dbl>          <dbl>     <dbl>         <dbl>\n 1 Jan       2004        230            132       230           126 \n 2 February  2004        230            134.      226.          128.\n 3 March     2004        230            137       225           131 \n 4 April     2004        234.           137       225           131 \n 5 May       2004        236            137       225           131 \n 6 June      2004        241            137       231.          134.\n 7 July      2004        241            137       234.          134.\n 8 August    2004        241            137       234.          134.\n 9 September 2004        241            136.      234.          130.\n10 October   2004        241            136.      234.          128.\n# … with 110 more rows\n\n\n\n\n\n\n\n\nPivot\n\n\n\nWe need to adjust this data so that it is long data (pivot). As of right now we can look at the data nicely, but can’t do much analysis across sizes because they are in different columns. I will shift this data to month, year, “carton_type” which will combine the 4 types of cartons into one column as their names and place their values into another column labeled “price”.\n\n\n\n\nCode\neggs_long <- eggs %>%\n  pivot_longer(cols=3:6,\n    names_to = c(\"carton_type\"),\n      values_to = \"Price\"\n  )\neggs_long\n\n\n# A tibble: 480 × 4\n   month    year  carton_type    Price\n   <chr>    <chr> <chr>          <dbl>\n 1 Jan      2004  xlarge_dzn      230 \n 2 Jan      2004  xlarge_halfdzn  132 \n 3 Jan      2004  large_dzn       230 \n 4 Jan      2004  large_halfdzn   126 \n 5 February 2004  xlarge_dzn      230 \n 6 February 2004  xlarge_halfdzn  134.\n 7 February 2004  large_dzn       226.\n 8 February 2004  large_halfdzn   128.\n 9 March    2004  xlarge_dzn      230 \n10 March    2004  xlarge_halfdzn  137 \n# … with 470 more rows\n\n\n\nAnalyze\nAlright, now our data is pretty succinct. We can start doing some analysis!\n\n\nCode\neggs_long %>%\n  arrange(desc(`Price`))\n\n\n# A tibble: 480 × 4\n   month    year  carton_type Price\n   <chr>    <chr> <chr>       <dbl>\n 1 November 2012  xlarge_dzn    290\n 2 December 2012  xlarge_dzn    290\n 3 Jan      2013  xlarge_dzn    290\n 4 February 2013  xlarge_dzn    290\n 5 March    2013  xlarge_dzn    290\n 6 April    2013  xlarge_dzn    290\n 7 May      2013  xlarge_dzn    290\n 8 June     2013  xlarge_dzn    290\n 9 July     2013  xlarge_dzn    290\n10 August   2013  xlarge_dzn    290\n# … with 470 more rows"
  },
  {
    "objectID": "posts/Sue-EllenDuffy_Challenge_3.html#price-analysis-by-carton-type",
    "href": "posts/Sue-EllenDuffy_Challenge_3.html#price-analysis-by-carton-type",
    "title": "Challenge 3: Eggs 2004-2013",
    "section": "Price Analysis by Carton Type",
    "text": "Price Analysis by Carton Type\n\n\nCode\neggs_long %>%\n  group_by(carton_type) %>%\n  summarise( Min = min(Price),\n             Max = max(Price),\n             Median = median(Price),\n             Mean = mean(Price)) %>%\n  arrange(desc(Mean))\n\n\n# A tibble: 4 × 5\n  carton_type      Min   Max Median  Mean\n  <chr>          <dbl> <dbl>  <dbl> <dbl>\n1 xlarge_dzn       230  290    286.  267.\n2 large_dzn        225  278.   268.  254.\n3 xlarge_halfdzn   132  188.   186.  164.\n4 large_halfdzn    126  178    174.  155.\n\n\nNo surprises here, xlarge_dzn is on average the most expensive of the 4 carton types and large_halfdzn is on average the least expensive of the 4 carton types."
  },
  {
    "objectID": "posts/Sue-EllenDuffy_Challenge_3.html#price-analysis-by-month",
    "href": "posts/Sue-EllenDuffy_Challenge_3.html#price-analysis-by-month",
    "title": "Challenge 3: Eggs 2004-2013",
    "section": "Price analysis by Month",
    "text": "Price analysis by Month\n\n\nCode\neggs_long %>%\n  group_by(month) %>%\n  summarise( Min = min(Price),\n             Max = max(Price),\n             Median = median(Price),\n             Mean =mean(Price)) %>%\n  arrange(desc(Mean)) %>%\n  print(n=12)\n\n\n# A tibble: 12 × 5\n   month       Min   Max Median  Mean\n   <chr>     <dbl> <dbl>  <dbl> <dbl>\n 1 July       128.   290   211.  212.\n 2 June       128.   290   210.  212.\n 3 December   128.   290   211.  212.\n 4 November   128.   290   211.  212.\n 5 August     128.   290   211.  212.\n 6 September  128.   290   211.  212.\n 7 October    128.   290   211.  212.\n 8 May        128.   290   207.  208.\n 9 April      128.   290   207.  208.\n10 March      128.   290   207.  207.\n11 February   128.   290   207.  207.\n12 Jan        126    290   209.  207.\n\n\nHere we see the most expensive months are at the beginning of the winter season and at the beginning of the summer season. I wonder if this has anything to do with egg laying production or if this increase is seen within the broader scope of food price statistics."
  },
  {
    "objectID": "posts/Sue-EllenDuffy_Challenge_3.html#price-analysis-by-year",
    "href": "posts/Sue-EllenDuffy_Challenge_3.html#price-analysis-by-year",
    "title": "Challenge 3: Eggs 2004-2013",
    "section": "Price Analysis by Year",
    "text": "Price Analysis by Year\n\n\nCode\neggs_long %>%\n  group_by(year) %>%\n  summarise( Min = min(Price),\n             Max = max(Price),\n             Median = median(Price),\n             Mean =mean(Price))\n\n\n# A tibble: 10 × 5\n   year    Min   Max Median  Mean\n   <chr> <dbl> <dbl>  <dbl> <dbl>\n 1 2004   126   241    181   183.\n 2 2005   128.  241    184.  185.\n 3 2006   128.  242.   184.  185.\n 4 2007   128.  245    186.  188.\n 5 2008   132   286.   211.  213.\n 6 2009   174.  286.   228.  230.\n 7 2010   174.  286.   226.  228.\n 8 2011   174.  286.   226.  229.\n 9 2012   173.  290    228.  229.\n10 2013   178   290    228.  231.\n\n\nWhen looking at the data by Year we are basically given the dimensions for: large half dozen price of the cheapest month in a given year (min) extra large dozen price of the most expensive month in a given year (max) average price across 4 carton types and 12 months in a given year(mean).This does allow us to look at the price fluctuations over the years.Two drastic year shifts are from 2007 to 2008 and 2008 to 2009."
  },
  {
    "objectID": "posts/Sue-EllenDuffy_Challenge_4.html",
    "href": "posts/Sue-EllenDuffy_Challenge_4.html",
    "title": "Challenge 4 Egg Data",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(readr)\nlibrary(readxl)\nlibrary(lubridate)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/Sue-EllenDuffy_Challenge_4.html#read-in-the-data",
    "href": "posts/Sue-EllenDuffy_Challenge_4.html#read-in-the-data",
    "title": "Challenge 4 Egg Data",
    "section": "Read in the Data",
    "text": "Read in the Data\n\n\nCode\neggs_og <- read_excel(\"_data/organiceggpoultry.xls\",\n                      sheet=\"Data\",\n                      range =cell_limits(c(6,2),c(NA,6)),\n                      col_names = c(\"date\", \"xlarge_dzn\", \"xlarge_halfdzn\", \"large_dzn\", \"large_halfdzn\")\n)\neggs_og\n\n\n# A tibble: 120 × 5\n   date      xlarge_dzn xlarge_halfdzn large_dzn large_halfdzn\n   <chr>          <dbl>          <dbl>     <dbl>         <dbl>\n 1 Jan 2004        230            132       230           126 \n 2 February        230            134.      226.          128.\n 3 March           230            137       225           131 \n 4 April           234.           137       225           131 \n 5 May             236            137       225           131 \n 6 June            241            137       231.          134.\n 7 July            241            137       234.          134.\n 8 August          241            137       234.          134.\n 9 September       241            136.      234.          130.\n10 October         241            136.      234.          128.\n# … with 110 more rows\n\n\n\n\n\n\n\n\nDate Format 1\n\n\n\nStarting off a little messy. Already I see this data is -wide- and the date needs formatting. Let’s see what other nuances might be lingering in the date column (count).\n\n\n\n\nCode\ntable(select(eggs_og, date))\n\n\ndate\n      April      August    December    February February /1    Jan 2004 \n         10          10          10           8           2           1 \n   Jan 2005    Jan 2006    Jan 2007    Jan 2008    Jan 2009    Jan 2010 \n          1           1           1           1           1           1 \n   Jan 2011    Jan 2012    Jan 2013        July        June       March \n          1           1           1          10          10          10 \n        May    November     October   September \n         10          10          10          10 \n\n\n\n\n\n\n\n\nDate Format 2\n\n\n\nIn the date column, January has a year indicator, 10 of the months appear 10 times, February appears 8 times and February/1 (a leap year) appears twice. We have to delete the /1 in February (mutate) and extend the year indicator from January to the rest of the months (separate and fill).\n\n\n\n\nCode\neggs <- eggs_og %>%\n  mutate(date = str_remove(date, \" /1\")) %>%\n  separate(date, into=c(\"month\", \"year\"), sep=\" \") %>%\n  fill(year)\neggs\n\n\n# A tibble: 120 × 6\n   month     year  xlarge_dzn xlarge_halfdzn large_dzn large_halfdzn\n   <chr>     <chr>      <dbl>          <dbl>     <dbl>         <dbl>\n 1 Jan       2004        230            132       230           126 \n 2 February  2004        230            134.      226.          128.\n 3 March     2004        230            137       225           131 \n 4 April     2004        234.           137       225           131 \n 5 May       2004        236            137       225           131 \n 6 June      2004        241            137       231.          134.\n 7 July      2004        241            137       234.          134.\n 8 August    2004        241            137       234.          134.\n 9 September 2004        241            136.      234.          130.\n10 October   2004        241            136.      234.          128.\n# … with 110 more rows\n\n\n\n\n\n\n\n\nDate Format 3\n\n\n\nWe need to mutate the Month and Year into one combined date. In order to do that we would mutate and make_date time. However when I try that I get an error. I’m not entirely sure why. I tried renaming Jan to January to see if that was the issue. It still won’t let me so I will just use the mutate and str_c functions to create the date column.\n\n\n\n\nCode\neggs<-eggs %>%\n  mutate(month=recode (month, 'Jan'='January')) %>%\n  mutate(date = str_c(month, year, sep=\" \"),\n         date = my(date))\nselect(eggs, month, year, date)\n\n\n# A tibble: 120 × 3\n   month     year  date      \n   <chr>     <chr> <date>    \n 1 January   2004  2004-01-01\n 2 February  2004  2004-02-01\n 3 March     2004  2004-03-01\n 4 April     2004  2004-04-01\n 5 May       2004  2004-05-01\n 6 June      2004  2004-06-01\n 7 July      2004  2004-07-01\n 8 August    2004  2004-08-01\n 9 September 2004  2004-09-01\n10 October   2004  2004-10-01\n# … with 110 more rows\n\n\n\n\nCode\neggs<-eggs%>%\n  mutate(date = make_datetime(month, year))\n\n\nError in `mutate()`:\nℹ In argument: `date = make_datetime(month, year)`.\nCaused by error:\n! Invalid input type, expected 'integer' actual 'character'"
  },
  {
    "objectID": "posts/Sue-EllenDuffy_Challenge_5.html",
    "href": "posts/Sue-EllenDuffy_Challenge_5.html",
    "title": "AirBnB New York 2019 Challenge 5",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(readr)\nlibrary(readxl)\nlibrary(lubridate)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(ggmap)\nlibrary(maps)\nlibrary(leaflet)\nlibrary(broom)\nlibrary(summarytools)\nlibrary(hrbrthemes)\nlibrary(plotly)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/Sue-EllenDuffy_Challenge_5.html#read-in-the-data",
    "href": "posts/Sue-EllenDuffy_Challenge_5.html#read-in-the-data",
    "title": "AirBnB New York 2019 Challenge 5",
    "section": "Read in the Data",
    "text": "Read in the Data\nI chose not to pivot this data because each listing was unique, even if a host had different listings, each had different price points, neighborhoods, room types, and names.\n\n\nCode\ndata<-read.csv(\"_data/AB_NYC_2019.csv\")\ntibble(data, 10)\n\n\n# A tibble: 48,895 × 17\n      id name      host_id host_…¹ neigh…² neigh…³ latit…⁴ longi…⁵ room_…⁶ price\n   <int> <chr>       <int> <chr>   <chr>   <chr>     <dbl>   <dbl> <chr>   <int>\n 1  2539 \"Clean &…    2787 John    Brookl… Kensin…    40.6   -74.0 Privat…   149\n 2  2595 \"Skylit …    2845 Jennif… Manhat… Midtown    40.8   -74.0 Entire…   225\n 3  3647 \"THE VIL…    4632 Elisab… Manhat… Harlem     40.8   -73.9 Privat…   150\n 4  3831 \"Cozy En…    4869 LisaRo… Brookl… Clinto…    40.7   -74.0 Entire…    89\n 5  5022 \"Entire …    7192 Laura   Manhat… East H…    40.8   -73.9 Entire…    80\n 6  5099 \"Large C…    7322 Chris   Manhat… Murray…    40.7   -74.0 Entire…   200\n 7  5121 \"BlissAr…    7356 Garon   Brookl… Bedfor…    40.7   -74.0 Privat…    60\n 8  5178 \"Large F…    8967 Shunic… Manhat… Hell's…    40.8   -74.0 Privat…    79\n 9  5203 \"Cozy Cl…    7490 MaryEl… Manhat… Upper …    40.8   -74.0 Privat…    79\n10  5238 \"Cute & …    7549 Ben     Manhat… Chinat…    40.7   -74.0 Entire…   150\n# … with 48,885 more rows, 7 more variables: minimum_nights <int>,\n#   number_of_reviews <int>, last_review <chr>, reviews_per_month <dbl>,\n#   calculated_host_listings_count <int>, availability_365 <int>, `10` <dbl>,\n#   and abbreviated variable names ¹​host_name, ²​neighbourhood_group,\n#   ³​neighbourhood, ⁴​latitude, ⁵​longitude, ⁶​room_type\n\n\nThe different data point for each listing:\n\n\nCode\ncolnames(data)\n\n\n [1] \"id\"                             \"name\"                          \n [3] \"host_id\"                        \"host_name\"                     \n [5] \"neighbourhood_group\"            \"neighbourhood\"                 \n [7] \"latitude\"                       \"longitude\"                     \n [9] \"room_type\"                      \"price\"                         \n[11] \"minimum_nights\"                 \"number_of_reviews\"             \n[13] \"last_review\"                    \"reviews_per_month\"             \n[15] \"calculated_host_listings_count\" \"availability_365\""
  },
  {
    "objectID": "posts/Sue-EllenDuffy_Challenge_5.html#univariate-graphs",
    "href": "posts/Sue-EllenDuffy_Challenge_5.html#univariate-graphs",
    "title": "AirBnB New York 2019 Challenge 5",
    "section": "Univariate Graphs:",
    "text": "Univariate Graphs:"
  },
  {
    "objectID": "posts/Sue-EllenDuffy_Challenge_5.html#bivariate-graph",
    "href": "posts/Sue-EllenDuffy_Challenge_5.html#bivariate-graph",
    "title": "AirBnB New York 2019 Challenge 5",
    "section": "Bivariate Graph:",
    "text": "Bivariate Graph:"
  },
  {
    "objectID": "posts/Sue-EllenDuffy_Challenge_5.html#maps---a-trial-and-error",
    "href": "posts/Sue-EllenDuffy_Challenge_5.html#maps---a-trial-and-error",
    "title": "AirBnB New York 2019 Challenge 5",
    "section": "Maps - A trial and error",
    "text": "Maps - A trial and error\nFor Fun! - anyone want to add onto it? I’d be curious to see what others could come up with."
  }
]