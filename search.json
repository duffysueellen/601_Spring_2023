[
  {
    "objectID": "about/Sue-EllenDuffy.html",
    "href": "about/Sue-EllenDuffy.html",
    "title": "Sue-Ellen Duffy",
    "section": "",
    "text": "UMass Amherst 2009-2013 Public Health - Community Health and Education\nUMass Amherst 2023-present DACSS\nFreelance Education Research (Observer, Assessor) - Abt Associates, Boston, MA\nAcademic Services Researcher - Jewish Vocational Services, Boston, MA\nFreelance Education Researcher (Assessor) - Harvard University, Boston, MA\nNonprofit Research and Food Justice Work - Fair Foods, Boston, MA\nFacilitation Trainer - UMass Amherst, MA"
  },
  {
    "objectID": "about/Sue-EllenDuffy.html#r-experience",
    "href": "about/Sue-EllenDuffy.html#r-experience",
    "title": "Sue-Ellen Duffy",
    "section": "R experience",
    "text": "R experience\nLimited - DACSS 601 is my first exposure!"
  },
  {
    "objectID": "about/Sue-EllenDuffy.html#research-interests",
    "href": "about/Sue-EllenDuffy.html#research-interests",
    "title": "Sue-Ellen Duffy",
    "section": "Research interests",
    "text": "Research interests\nEducation, Gender, Food Justice/Access, Connection, PAR"
  },
  {
    "objectID": "about/Sue-EllenDuffy.html#hometown",
    "href": "about/Sue-EllenDuffy.html#hometown",
    "title": "Sue-Ellen Duffy",
    "section": "Hometown",
    "text": "Hometown\nI grew up in The South Shore of Massachusetts and have lived in Boston now for 7 years."
  },
  {
    "objectID": "about/Sue-EllenDuffy.html#hobbies",
    "href": "about/Sue-EllenDuffy.html#hobbies",
    "title": "Sue-Ellen Duffy",
    "section": "Hobbies",
    "text": "Hobbies\nForaging, Gardening, Sunbasking, Bar Pizza"
  },
  {
    "objectID": "about/Sue-EllenDuffy.html#fun-fact",
    "href": "about/Sue-EllenDuffy.html#fun-fact",
    "title": "Sue-Ellen Duffy",
    "section": "Fun fact",
    "text": "Fun fact\nI once went on tour with the band They Might Be Giants - I was their truck driver and assistant manager ;)"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Contributors",
    "section": "",
    "text": "Find out more about our DACSS students who contributed to the blog.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSue-Ellen Duffy\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DACSS 601: Data Science Fundamentals Spring-2023",
    "section": "",
    "text": "The blog posts here are contributed by students enrolled in DACSS 601, Fundamentals of Data Science. The course provides students with an introduction to R and the tidyverse, scientific publishing, and collaboration through GitHub, building a foundation for future coursework. Students also are introduced to general data management and data wrangling skills, with an emphasis on best practice workflows and tidy data management.\n\n\n\n\n\n\n\n\n\n\nFinal Project Assignment#2: Sue-Ellen Duffy\n\n\n\n\n\n\n\nfinal_Project_assignment_2\n\n\nfinal_project_data_visualization\n\n\n\n\nExploratory Analysis and Visualization\n\n\n\n\n\n\nMay 17, 2023\n\n\nSue-Ellen Duffy\n\n\n\n\n\n\n\n\nChallenge 8\n\n\n\n\n\n\n\nchallenge_8\n\n\nrailroads\n\n\nsnl\n\n\nfaostat\n\n\ndebt\n\n\n\n\nJoining Data\n\n\n\n\n\n\nApr 27, 2023\n\n\nSue-Ellen Duffy\n\n\n\n\n\n\n\n\nChallenge 7 AirBnB Data with Modified Maps\n\n\n\n\n\n\n\nchallenge_7\n\n\nair_bnb\n\n\nSue-Ellen Duffy\n\n\n\n\nVisualizing Multiple Dimensions\n\n\n\n\n\n\nApr 19, 2023\n\n\nSue-Ellen Duffy\n\n\n\n\n\n\n\n\nFinal Project Assignment#1: Sue-Ellen Duffy\n\n\n\n\n\n\n\nfinal_Project_assignment_1\n\n\nfinal_project_data_description\n\n\n\n\nProject & Data Description\n\n\n\n\n\n\nApr 11, 2023\n\n\nSue-Ellen Duffy\n\n\n\n\n\n\n\n\nChallenge 6 Air BnB Data by Last Review\n\n\n\n\n\n\n\nchallenge_6\n\n\nSue-Ellen Duffy\n\n\nair_bnb\n\n\n\n\nVisualizing Time and Relationships\n\n\n\n\n\n\nApr 2, 2023\n\n\nSue-Ellen Duffy\n\n\n\n\n\n\n\n\nAirBnB New York 2019 Challenge 5\n\n\n\n\n\n\n\nchallenge_5\n\n\nSue-Ellen Duffy\n\n\nairbnb\n\n\n\n\nPlotting Price per Neighbourhood Groups\n\n\n\n\n\n\nMar 25, 2023\n\n\nSue-Ellen Duffy\n\n\n\n\n\n\n\n\nChallenge 4 Egg Data\n\n\n\n\n\n\n\nchallenge_4\n\n\nSue-Ellen Duffy\n\n\neggs\n\n\n\n\nData wrangling: Mutate\n\n\n\n\n\n\nMar 22, 2023\n\n\nSue-Ellen Duffy\n\n\n\n\n\n\n\n\nChallenge 3: Eggs 2004-2013\n\n\n\n\n\n\n\nchallenge_3\n\n\neggs\n\n\nSue-Ellen Duffy\n\n\n\n\nPivoting Egg Data\n\n\n\n\n\n\nMar 14, 2023\n\n\nSue-Ellen Duffy\n\n\n\n\n\n\n\n\nUnderstanding the FAOSTAT Country Codes\n\n\n\n\n\n\n\nchallenge_2\n\n\nSue-Ellen Duffy\n\n\nFAOSTAT\n\n\n\n\n\n\n\n\n\n\n\nFeb 24, 2023\n\n\nSue-Ellen Duffy\n\n\n\n\n\n\n\n\nRailroad Employees Challenge 1\n\n\n\n\n\n\n\nchallenge_1\n\n\nSue-Ellen Duffy\n\n\nRailroad Employee Dataset\n\n\n\n\n\n\n\n\n\n\n\nFeb 23, 2023\n\n\nSue-Ellen Duffy\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/SDuffy_challenge_1.html",
    "href": "posts/SDuffy_challenge_1.html",
    "title": "Railroad Employees Challenge 1",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(summarytools)\n\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/SDuffy_challenge_1.html#reading-in-the-data",
    "href": "posts/SDuffy_challenge_1.html#reading-in-the-data",
    "title": "Railroad Employees Challenge 1",
    "section": "Reading in the Data",
    "text": "Reading in the Data\nI analyzed the “railroad_2012_county_clean.csv” data for Challenge 1. This data describes the Total Number of Railroad Employees by County and State in the United States in 2012. Upon first glance the data contains 3 columns and 2,930 rows. The columns are: state, county, and total_employees\n\n\nCode\n#Read in data and rename railroad_2012_clean_county as data\ndata <- rename(read_csv(\"_data/railroad_2012_clean_county.csv\"))\n\n\nRows: 2930 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): state, county\ndbl (1): total_employees\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\n#Preview data \ndata\n\n\n# A tibble: 2,930 × 3\n   state county               total_employees\n   <chr> <chr>                          <dbl>\n 1 AE    APO                                2\n 2 AK    ANCHORAGE                          7\n 3 AK    FAIRBANKS NORTH STAR               2\n 4 AK    JUNEAU                             3\n 5 AK    MATANUSKA-SUSITNA                  2\n 6 AK    SITKA                              1\n 7 AK    SKAGWAY MUNICIPALITY              88\n 8 AL    AUTAUGA                          102\n 9 AL    BALDWIN                          143\n10 AL    BARBOUR                            1\n# … with 2,920 more rows"
  },
  {
    "objectID": "posts/SDuffy_challenge_1.html#summary-of-data",
    "href": "posts/SDuffy_challenge_1.html#summary-of-data",
    "title": "Railroad Employees Challenge 1",
    "section": "Summary of Data",
    "text": "Summary of Data\nRunning the dfsummary(data) function shows us:\n\nThe data is complete: there are no missing data.\nTop ten states ranked with the most counties. Texas has the most counties of any other state, accounting for 7.5% of all counties in the United States.\nThere are multiples of county names. We see in the following graph the top 10 county names that are used in the United States. (There are 31 Washington county names in this data plot, that’s far more than I thought there were in the United States!)\n\n\n\nCode\ndfSummary(data)\n\n\nData Frame Summary  \ndata  \nDimensions: 2930 x 3  \nDuplicates: 0  \n\n-----------------------------------------------------------------------------------------------------------------\nNo   Variable          Stats / Values             Freqs (% of Valid)    Graph                Valid      Missing  \n---- ----------------- -------------------------- --------------------- -------------------- ---------- ---------\n1    state             1. TX                       221 ( 7.5%)          I                    2930       0        \n     [character]       2. GA                       152 ( 5.2%)          I                    (100.0%)   (0.0%)   \n                       3. KY                       119 ( 4.1%)                                                   \n                       4. MO                       115 ( 3.9%)                                                   \n                       5. IL                       103 ( 3.5%)                                                   \n                       6. IA                        99 ( 3.4%)                                                   \n                       7. KS                        95 ( 3.2%)                                                   \n                       8. NC                        94 ( 3.2%)                                                   \n                       9. IN                        92 ( 3.1%)                                                   \n                       10. VA                       92 ( 3.1%)                                                   \n                       [ 43 others ]              1748 (59.7%)          IIIIIIIIIII                              \n\n2    county            1. WASHINGTON                31 ( 1.1%)                               2930       0        \n     [character]       2. JEFFERSON                 26 ( 0.9%)                               (100.0%)   (0.0%)   \n                       3. FRANKLIN                  24 ( 0.8%)                                                   \n                       4. LINCOLN                   24 ( 0.8%)                                                   \n                       5. JACKSON                   22 ( 0.8%)                                                   \n                       6. MADISON                   19 ( 0.6%)                                                   \n                       7. MONTGOMERY                18 ( 0.6%)                                                   \n                       8. CLAY                      17 ( 0.6%)                                                   \n                       9. MARION                    17 ( 0.6%)                                                   \n                       10. MONROE                   17 ( 0.6%)                                                   \n                       [ 1699 others ]            2715 (92.7%)          IIIIIIIIIIIIIIIIII                       \n\n3    total_employees   Mean (sd) : 87.2 (283.6)   404 distinct values   :                    2930       0        \n     [numeric]         min < med < max:                                 :                    (100.0%)   (0.0%)   \n                       1 < 21 < 8207                                    :                                        \n                       IQR (CV) : 58 (3.3)                              :                                        \n                                                                        :                                        \n-----------------------------------------------------------------------------------------------------------------\n\n\n\n\nCode\n#How many states are represented in the data?\ndata %>%\n  select(state) %>%\n  n_distinct(.)\n\n\n[1] 53\n\n\nThere are only 50 recognized states, so we need to dig a little deeper to find out what the three additional ‘states’ represent.\n\n\nCode\n#Show unique state data\nunique(data$state)\n\n\n [1] \"AE\" \"AK\" \"AL\" \"AP\" \"AR\" \"AZ\" \"CA\" \"CO\" \"CT\" \"DC\" \"DE\" \"FL\" \"GA\" \"HI\" \"IA\"\n[16] \"ID\" \"IL\" \"IN\" \"KS\" \"KY\" \"LA\" \"MA\" \"MD\" \"ME\" \"MI\" \"MN\" \"MO\" \"MS\" \"MT\" \"NC\"\n[31] \"ND\" \"NE\" \"NH\" \"NJ\" \"NM\" \"NV\" \"NY\" \"OH\" \"OK\" \"OR\" \"PA\" \"RI\" \"SC\" \"SD\" \"TN\"\n[46] \"TX\" \"UT\" \"VA\" \"VT\" \"WA\" \"WI\" \"WV\" \"WY\"\n\n\nAE, AP, and DC are the three non-states cases. AE and AP are military addresses. DC is Washington DC."
  },
  {
    "objectID": "posts/Sue-EllenDuffy_Challenge_2.html",
    "href": "posts/Sue-EllenDuffy_Challenge_2.html",
    "title": "Understanding the FAOSTAT Country Codes",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(summarytools)\nlibrary(readxl)\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/Sue-EllenDuffy_Challenge_2.html#faostat-data",
    "href": "posts/Sue-EllenDuffy_Challenge_2.html#faostat-data",
    "title": "Understanding the FAOSTAT Country Codes",
    "section": "FAOSTAT data",
    "text": "FAOSTAT data\nThis data set is simply the Country Profiles for the Food and Agriculture Organization Corporate Statistical Database (FAOSTAT).\nIn this set of data, each column is describing the same data, but vary in who is describing or using that data. The United Nations Terminology Database, the Statistics Division of the United Nations Secretariat, and the International Organization for Standardization each have different ways of coding the same countries, so this database helps us understand which country or region or group of countries is being described in other FAO data sets.\n\n\nCode\n#Read in data and rename FAOSTAT_country_groups as groups\ndata <- read_csv(\"_data/FAOSTAT_country_groups.csv\")\n\n\nRows: 1943 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): Country Group, Country, M49 Code, ISO2 Code, ISO3 Code\ndbl (2): Country Group Code, Country Code\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\ndata\n\n\n# A tibble: 1,943 × 7\n   `Country Group Code` `Country Group` Countr…¹ Country M49 C…² ISO2 …³ ISO3 …⁴\n                  <dbl> <chr>              <dbl> <chr>   <chr>   <chr>   <chr>  \n 1                 5100 Africa                 4 Algeria 012     DZ      DZA    \n 2                 5100 Africa                 7 Angola  024     AO      AGO    \n 3                 5100 Africa                53 Benin   204     BJ      BEN    \n 4                 5100 Africa                20 Botswa… 072     BW      BWA    \n 5                 5100 Africa               233 Burkin… 854     BF      BFA    \n 6                 5100 Africa                29 Burundi 108     BI      BDI    \n 7                 5100 Africa                35 Cabo V… 132     CV      CPV    \n 8                 5100 Africa                32 Camero… 120     CM      CMR    \n 9                 5100 Africa                37 Centra… 140     CF      CAF    \n10                 5100 Africa                39 Chad    148     TD      TCD    \n# … with 1,933 more rows, and abbreviated variable names ¹​`Country Code`,\n#   ²​`M49 Code`, ³​`ISO2 Code`, ⁴​`ISO3 Code`"
  },
  {
    "objectID": "posts/Sue-EllenDuffy_Challenge_2.html#summarize-the-data",
    "href": "posts/Sue-EllenDuffy_Challenge_2.html#summarize-the-data",
    "title": "Understanding the FAOSTAT Country Codes",
    "section": "Summarize the data",
    "text": "Summarize the data\nLooking through this quick summary we see that there are 277 Countries in this data, that IS02 is missing data, that the M49 Code column is characterized as being characters (though they are all numeric) and while there are other data here, there are two charts of interests - Country Group and Country.\nIS02 is “missing data” because of the code NA which is their 2-alpha code for Nambia.\nM49 Code column is characterized as being characters even though they are all numeric.\nThe Country Group tibble shows us the 10 Country Groups containing the most Countries.\nThe Country tibble shows us the 10 Countries that were categorized into different Country Groups the most.\n\n\nCode\ndfSummary(data)\n\n\nData Frame Summary  \ndata  \nDimensions: 1943 x 7  \nDuplicates: 0  \n\n--------------------------------------------------------------------------------------------------------------------------\nNo   Variable             Stats / Values                  Freqs (% of Valid)    Graph                 Valid      Missing  \n---- -------------------- ------------------------------- --------------------- --------------------- ---------- ---------\n1    Country Group Code   Mean (sd) : 5501.8 (1356.5)     58 distinct values              :           1943       0        \n     [numeric]            min < med < max:                                                : :         (100.0%)   (0.0%)   \n                          336 < 5403 < 9011                                               : :                             \n                          IQR (CV) : 648 (0.2)                                            : :                             \n                                                                                .         : :     .                       \n\n2    Country Group        1. World                        277 (14.3%)           II                    1943       0        \n     [character]          2. Non-Annex I countries        161 ( 8.3%)           I                     (100.0%)   (0.0%)   \n                          3. Net Food Importing Develo     81 ( 4.2%)                                                     \n                          4. Annex I countries             78 ( 4.0%)                                                     \n                          5. High-income economies         64 ( 3.3%)                                                     \n                          6. Africa                        63 ( 3.2%)                                                     \n                          7. Europe                        63 ( 3.2%)                                                     \n                          8. Americas                      61 ( 3.1%)                                                     \n                          9. Small Island Developing S     58 ( 3.0%)                                                     \n                          10. Upper-middle-income econo    56 ( 2.9%)                                                     \n                          [ 48 others ]                   981 (50.5%)           IIIIIIIIII                                \n\n3    Country Code         Mean (sd) : 142.2 (96.4)        277 distinct values   : : : :               1943       0        \n     [numeric]            min < med < max:                                      : : : :               (100.0%)   (0.0%)   \n                          1 < 136 < 622                                         : : : :                                   \n                          IQR (CV) : 133 (0.7)                                  : : : : .                                 \n                                                                                : : : : :         .                       \n\n4    Country              1. Afghanistan                    11 ( 0.6%)                                1943       0        \n     [character]          2. Burkina Faso                   11 ( 0.6%)                                (100.0%)   (0.0%)   \n                          3. Burundi                        11 ( 0.6%)                                                    \n                          4. Central African Republic       11 ( 0.6%)                                                    \n                          5. Chad                           11 ( 0.6%)                                                    \n                          6. Comoros                        11 ( 0.6%)                                                    \n                          7. Ethiopia                       11 ( 0.6%)                                                    \n                          8. Guinea-Bissau                  11 ( 0.6%)                                                    \n                          9. Lesotho                        11 ( 0.6%)                                                    \n                          10. Malawi                        11 ( 0.6%)                                                    \n                          [ 267 others ]                  1833 (94.3%)          IIIIIIIIIIIIIIIIII                        \n\n5    M49 Code             1. 004                            11 ( 0.6%)                                1943       0        \n     [character]          2. 108                            11 ( 0.6%)                                (100.0%)   (0.0%)   \n                          3. 140                            11 ( 0.6%)                                                    \n                          4. 148                            11 ( 0.6%)                                                    \n                          5. 174                            11 ( 0.6%)                                                    \n                          6. 231                            11 ( 0.6%)                                                    \n                          7. 426                            11 ( 0.6%)                                                    \n                          8. 454                            11 ( 0.6%)                                                    \n                          9. 466                            11 ( 0.6%)                                                    \n                          10. 496                           11 ( 0.6%)                                                    \n                          [ 267 others ]                  1833 (94.3%)          IIIIIIIIIIIIIIIIII                        \n\n6    ISO2 Code            1. AF                             11 ( 0.6%)                                1935       8        \n     [character]          2. BF                             11 ( 0.6%)                                (99.6%)    (0.4%)   \n                          3. BI                             11 ( 0.6%)                                                    \n                          4. CF                             11 ( 0.6%)                                                    \n                          5. ET                             11 ( 0.6%)                                                    \n                          6. GW                             11 ( 0.6%)                                                    \n                          7. KM                             11 ( 0.6%)                                                    \n                          8. LS                             11 ( 0.6%)                                                    \n                          9. ML                             11 ( 0.6%)                                                    \n                          10. MN                            11 ( 0.6%)                                                    \n                          [ 266 others ]                  1825 (94.3%)          IIIIIIIIIIIIIIIIII                        \n\n7    ISO3 Code            1. AFG                            11 ( 0.6%)                                1943       0        \n     [character]          2. BDI                            11 ( 0.6%)                                (100.0%)   (0.0%)   \n                          3. BFA                            11 ( 0.6%)                                                    \n                          4. CAF                            11 ( 0.6%)                                                    \n                          5. COM                            11 ( 0.6%)                                                    \n                          6. ETH                            11 ( 0.6%)                                                    \n                          7. GNB                            11 ( 0.6%)                                                    \n                          8. LSO                            11 ( 0.6%)                                                    \n                          9. MLI                            11 ( 0.6%)                                                    \n                          10. MNG                           11 ( 0.6%)                                                    \n                          [ 267 others ]                  1833 (94.3%)          IIIIIIIIIIIIIIIIII                        \n--------------------------------------------------------------------------------------------------------------------------"
  },
  {
    "objectID": "posts/Sue-EllenDuffy_Challenge_2.html#how-are-the-codes-different",
    "href": "posts/Sue-EllenDuffy_Challenge_2.html#how-are-the-codes-different",
    "title": "Understanding the FAOSTAT Country Codes",
    "section": "How are the codes different?",
    "text": "How are the codes different?\nTake the United States of America for example. When filtering for “United States of America” we come out with 8 different rows of data. The Country Code, M49 Code, IS02 Code and IS03 Codes while unique to their specifics are unchanged for the 8 rows. The difference here, and why we get 8 different rows for the United States of America, is that their Country Group Code and Country Group are different. The Country Group Code is simply the number associated with the Country Group. It appears that Country Group is a categorical code, listing the USA as being part of the Americas, High-income economies, North and Central America, Annex I countries, etc.\nThe Country Group would allow for quick and categorical data analysis, such as analyzing the countries by economics (high-income economies and low-income economies) or by region (Northern and Central America to the Americas.\n\n\nCode\nUSA <- filter(data, `Country` == \"United States of America\")\nUSA\n\n\n# A tibble: 8 × 7\n  `Country Group Code` `Country Group`   Count…¹ Country M49 C…² ISO2 …³ ISO3 …⁴\n                 <dbl> <chr>               <dbl> <chr>   <chr>   <chr>   <chr>  \n1                 5200 Americas              231 United… 840     US      USA    \n2                 5848 Annex I countries     231 United… 840     US      USA    \n3                 9010 High-income econ…     231 United… 840     US      USA    \n4                  336 North and Centra…     231 United… 840     US      USA    \n5                 5203 Northern America      231 United… 840     US      USA    \n6                 5208 Northern America…     231 United… 840     US      USA    \n7                 5873 OECD                  231 United… 840     US      USA    \n8                 5000 World                 231 United… 840     US      USA    \n# … with abbreviated variable names ¹​`Country Code`, ²​`M49 Code`, ³​`ISO2 Code`,\n#   ⁴​`ISO3 Code`"
  },
  {
    "objectID": "posts/Sue-EllenDuffy_Challenge_2.html#the-world",
    "href": "posts/Sue-EllenDuffy_Challenge_2.html#the-world",
    "title": "Understanding the FAOSTAT Country Codes",
    "section": "The “World”",
    "text": "The “World”\nThe one Country Group that contains all of the countries is “World”, which consists of 277 Countries. If the goal was to analyze all the countries at once, the filter should be set to “World”.\n\n\nCode\nworld <- filter(data, `Country Group` == \"World\")\nworld\n\n\n# A tibble: 277 × 7\n   `Country Group Code` `Country Group` Countr…¹ Country M49 C…² ISO2 …³ ISO3 …⁴\n                  <dbl> <chr>              <dbl> <chr>   <chr>   <chr>   <chr>  \n 1                 5000 World                  2 Afghan… 004     AF      AFG    \n 2                 5000 World                284 Åland … 248     F284    ALA    \n 3                 5000 World                  3 Albania 008     AL      ALB    \n 4                 5000 World                  4 Algeria 012     DZ      DZA    \n 5                 5000 World                  5 Americ… 016     AS      ASM    \n 6                 5000 World                  6 Andorra 020     AD      AND    \n 7                 5000 World                  7 Angola  024     AO      AGO    \n 8                 5000 World                258 Anguil… 660     AI      AIA    \n 9                 5000 World                 30 Antarc… 010     AQ      ATA    \n10                 5000 World                  8 Antigu… 028     AG      ATG    \n# … with 267 more rows, and abbreviated variable names ¹​`Country Code`,\n#   ²​`M49 Code`, ³​`ISO2 Code`, ⁴​`ISO3 Code`"
  },
  {
    "objectID": "posts/Sue-EllenDuffy_Challenge_2.html#anything-else-interesting",
    "href": "posts/Sue-EllenDuffy_Challenge_2.html#anything-else-interesting",
    "title": "Understanding the FAOSTAT Country Codes",
    "section": "Anything Else Interesting?",
    "text": "Anything Else Interesting?\nThe M49 Codes with the suffix “.01” are characterized as being “unspecified (population)”. I’m not entirely sure what that means, so it could be interesting to understand this further. Here is one example:\n\n\nCode\ndata %>%\n  filter(`M49 Code` == \"155.01\") %>%\nselect(\"Country Group\", \"Country\", \"M49 Code\")\n\n\n# A tibble: 3 × 3\n  `Country Group` Country                                  `M49 Code`\n  <chr>           <chr>                                    <chr>     \n1 Europe          Western Europe, unspecified (population) 155.01    \n2 Western Europe  Western Europe, unspecified (population) 155.01    \n3 World           Western Europe, unspecified (population) 155.01"
  },
  {
    "objectID": "posts/Sue-EllenDuffy_Challenge_3.html",
    "href": "posts/Sue-EllenDuffy_Challenge_3.html",
    "title": "Challenge 3: Eggs 2004-2013",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(summarytools)\nlibrary(readxl)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/Sue-EllenDuffy_Challenge_3.html#reading-in-the-egg-data",
    "href": "posts/Sue-EllenDuffy_Challenge_3.html#reading-in-the-egg-data",
    "title": "Challenge 3: Eggs 2004-2013",
    "section": "Reading in the egg data",
    "text": "Reading in the egg data\n\n\nCode\neggs_og <- read_excel(\"_data/organiceggpoultry.xls\",\n                      sheet=\"Data\",\n                      range =cell_limits(c(6,2),c(NA,6)),\n                      col_names = c(\"date\", \"xlarge_dzn\", \"xlarge_halfdzn\", \"large_dzn\", \"large_halfdzn\")\n)\neggs_og\n\n\n# A tibble: 120 × 5\n   date      xlarge_dzn xlarge_halfdzn large_dzn large_halfdzn\n   <chr>          <dbl>          <dbl>     <dbl>         <dbl>\n 1 Jan 2004        230            132       230           126 \n 2 February        230            134.      226.          128.\n 3 March           230            137       225           131 \n 4 April           234.           137       225           131 \n 5 May             236            137       225           131 \n 6 June            241            137       231.          134.\n 7 July            241            137       234.          134.\n 8 August          241            137       234.          134.\n 9 September       241            136.      234.          130.\n10 October         241            136.      234.          128.\n# … with 110 more rows\n\n\n\n\n\n\n\n\nDate Format 1\n\n\n\nStarting off a little messy. Already I see this data is -wide- and the date needs formatting. Let’s see what other nuances might be lingering in the date column (count).\n\n\n\n\nCode\ntable(select(eggs_og, date))\n\n\ndate\n      April      August    December    February February /1    Jan 2004 \n         10          10          10           8           2           1 \n   Jan 2005    Jan 2006    Jan 2007    Jan 2008    Jan 2009    Jan 2010 \n          1           1           1           1           1           1 \n   Jan 2011    Jan 2012    Jan 2013        July        June       March \n          1           1           1          10          10          10 \n        May    November     October   September \n         10          10          10          10 \n\n\n\n\n\n\n\n\nDate Format 2\n\n\n\nIn the date column, January has a year indicator, 10 of the months appear 10 times, February appears 8 times and February/1 (a leap year) appears twice. We have to delete the /1 in February (mutate) and extend the year indicator from January to the rest of the months (separate and fill).\n\n\n\n\nCode\neggs <- eggs_og %>%\n  mutate(date = str_remove(date, \" /1\")) %>%\n  separate(date,into=c(\"month\", \"year\"), sep=\" \") %>%\n  fill(year)\neggs\n\n\n# A tibble: 120 × 6\n   month     year  xlarge_dzn xlarge_halfdzn large_dzn large_halfdzn\n   <chr>     <chr>      <dbl>          <dbl>     <dbl>         <dbl>\n 1 Jan       2004        230            132       230           126 \n 2 February  2004        230            134.      226.          128.\n 3 March     2004        230            137       225           131 \n 4 April     2004        234.           137       225           131 \n 5 May       2004        236            137       225           131 \n 6 June      2004        241            137       231.          134.\n 7 July      2004        241            137       234.          134.\n 8 August    2004        241            137       234.          134.\n 9 September 2004        241            136.      234.          130.\n10 October   2004        241            136.      234.          128.\n# … with 110 more rows\n\n\n\n\n\n\n\n\nPivot\n\n\n\nWe need to adjust this data so that it is long data (pivot). As of right now we can look at the data nicely, but can’t do much analysis across sizes because they are in different columns. I will shift this data to month, year, “carton_type” which will combine the 4 types of cartons into one column as their names and place their values into another column labeled “price”.\n\n\n\n\nCode\neggs_long <- eggs %>%\n  pivot_longer(cols=3:6,\n    names_to = c(\"carton_type\"),\n      values_to = \"Price\"\n  )\neggs_long\n\n\n# A tibble: 480 × 4\n   month    year  carton_type    Price\n   <chr>    <chr> <chr>          <dbl>\n 1 Jan      2004  xlarge_dzn      230 \n 2 Jan      2004  xlarge_halfdzn  132 \n 3 Jan      2004  large_dzn       230 \n 4 Jan      2004  large_halfdzn   126 \n 5 February 2004  xlarge_dzn      230 \n 6 February 2004  xlarge_halfdzn  134.\n 7 February 2004  large_dzn       226.\n 8 February 2004  large_halfdzn   128.\n 9 March    2004  xlarge_dzn      230 \n10 March    2004  xlarge_halfdzn  137 \n# … with 470 more rows\n\n\n\nAnalyze\nAlright, now our data is pretty succinct. We can start doing some analysis!\n\n\nCode\neggs_long %>%\n  arrange(desc(`Price`))\n\n\n# A tibble: 480 × 4\n   month    year  carton_type Price\n   <chr>    <chr> <chr>       <dbl>\n 1 November 2012  xlarge_dzn    290\n 2 December 2012  xlarge_dzn    290\n 3 Jan      2013  xlarge_dzn    290\n 4 February 2013  xlarge_dzn    290\n 5 March    2013  xlarge_dzn    290\n 6 April    2013  xlarge_dzn    290\n 7 May      2013  xlarge_dzn    290\n 8 June     2013  xlarge_dzn    290\n 9 July     2013  xlarge_dzn    290\n10 August   2013  xlarge_dzn    290\n# … with 470 more rows"
  },
  {
    "objectID": "posts/Sue-EllenDuffy_Challenge_3.html#price-analysis-by-carton-type",
    "href": "posts/Sue-EllenDuffy_Challenge_3.html#price-analysis-by-carton-type",
    "title": "Challenge 3: Eggs 2004-2013",
    "section": "Price Analysis by Carton Type",
    "text": "Price Analysis by Carton Type\n\n\nCode\neggs_long %>%\n  group_by(carton_type) %>%\n  summarise( Min = min(Price),\n             Max = max(Price),\n             Median = median(Price),\n             Mean = mean(Price)) %>%\n  arrange(desc(Mean))\n\n\n# A tibble: 4 × 5\n  carton_type      Min   Max Median  Mean\n  <chr>          <dbl> <dbl>  <dbl> <dbl>\n1 xlarge_dzn       230  290    286.  267.\n2 large_dzn        225  278.   268.  254.\n3 xlarge_halfdzn   132  188.   186.  164.\n4 large_halfdzn    126  178    174.  155.\n\n\nNo surprises here, xlarge_dzn is on average the most expensive of the 4 carton types and large_halfdzn is on average the least expensive of the 4 carton types."
  },
  {
    "objectID": "posts/Sue-EllenDuffy_Challenge_3.html#price-analysis-by-month",
    "href": "posts/Sue-EllenDuffy_Challenge_3.html#price-analysis-by-month",
    "title": "Challenge 3: Eggs 2004-2013",
    "section": "Price analysis by Month",
    "text": "Price analysis by Month\n\n\nCode\neggs_long %>%\n  group_by(month) %>%\n  summarise( Min = min(Price),\n             Max = max(Price),\n             Median = median(Price),\n             Mean =mean(Price)) %>%\n  arrange(desc(Mean)) %>%\n  print(n=12)\n\n\n# A tibble: 12 × 5\n   month       Min   Max Median  Mean\n   <chr>     <dbl> <dbl>  <dbl> <dbl>\n 1 July       128.   290   211.  212.\n 2 June       128.   290   210.  212.\n 3 December   128.   290   211.  212.\n 4 November   128.   290   211.  212.\n 5 August     128.   290   211.  212.\n 6 September  128.   290   211.  212.\n 7 October    128.   290   211.  212.\n 8 May        128.   290   207.  208.\n 9 April      128.   290   207.  208.\n10 March      128.   290   207.  207.\n11 February   128.   290   207.  207.\n12 Jan        126    290   209.  207.\n\n\nHere we see the most expensive months are at the beginning of the winter season and at the beginning of the summer season. I wonder if this has anything to do with egg laying production or if this increase is seen within the broader scope of food price statistics."
  },
  {
    "objectID": "posts/Sue-EllenDuffy_Challenge_3.html#price-analysis-by-year",
    "href": "posts/Sue-EllenDuffy_Challenge_3.html#price-analysis-by-year",
    "title": "Challenge 3: Eggs 2004-2013",
    "section": "Price Analysis by Year",
    "text": "Price Analysis by Year\n\n\nCode\neggs_long %>%\n  group_by(year) %>%\n  summarise( Min = min(Price),\n             Max = max(Price),\n             Median = median(Price),\n             Mean =mean(Price))\n\n\n# A tibble: 10 × 5\n   year    Min   Max Median  Mean\n   <chr> <dbl> <dbl>  <dbl> <dbl>\n 1 2004   126   241    181   183.\n 2 2005   128.  241    184.  185.\n 3 2006   128.  242.   184.  185.\n 4 2007   128.  245    186.  188.\n 5 2008   132   286.   211.  213.\n 6 2009   174.  286.   228.  230.\n 7 2010   174.  286.   226.  228.\n 8 2011   174.  286.   226.  229.\n 9 2012   173.  290    228.  229.\n10 2013   178   290    228.  231.\n\n\nWhen looking at the data by Year we are basically given the dimensions for: large half dozen price of the cheapest month in a given year (min) extra large dozen price of the most expensive month in a given year (max) average price across 4 carton types and 12 months in a given year(mean).This does allow us to look at the price fluctuations over the years.Two drastic year shifts are from 2007 to 2008 and 2008 to 2009."
  },
  {
    "objectID": "posts/Sue-EllenDuffy_Challenge_4.html",
    "href": "posts/Sue-EllenDuffy_Challenge_4.html",
    "title": "Challenge 4 Egg Data",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(readr)\nlibrary(readxl)\nlibrary(lubridate)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/Sue-EllenDuffy_Challenge_4.html#read-in-the-data",
    "href": "posts/Sue-EllenDuffy_Challenge_4.html#read-in-the-data",
    "title": "Challenge 4 Egg Data",
    "section": "Read in the Data",
    "text": "Read in the Data\n\n\nCode\neggs_og <- read_excel(\"_data/organiceggpoultry.xls\",\n                      sheet=\"Data\",\n                      range =cell_limits(c(6,2),c(NA,6)),\n                      col_names = c(\"date\", \"xlarge_dzn\", \"xlarge_halfdzn\", \"large_dzn\", \"large_halfdzn\")\n)\neggs_og\n\n\n# A tibble: 120 × 5\n   date      xlarge_dzn xlarge_halfdzn large_dzn large_halfdzn\n   <chr>          <dbl>          <dbl>     <dbl>         <dbl>\n 1 Jan 2004        230            132       230           126 \n 2 February        230            134.      226.          128.\n 3 March           230            137       225           131 \n 4 April           234.           137       225           131 \n 5 May             236            137       225           131 \n 6 June            241            137       231.          134.\n 7 July            241            137       234.          134.\n 8 August          241            137       234.          134.\n 9 September       241            136.      234.          130.\n10 October         241            136.      234.          128.\n# … with 110 more rows\n\n\n\n\n\n\n\n\nDate Format 1\n\n\n\nStarting off a little messy. Already I see this data is -wide- and the date needs formatting. Let’s see what other nuances might be lingering in the date column (count).\n\n\n\n\nCode\ntable(select(eggs_og, date))\n\n\ndate\n      April      August    December    February February /1    Jan 2004 \n         10          10          10           8           2           1 \n   Jan 2005    Jan 2006    Jan 2007    Jan 2008    Jan 2009    Jan 2010 \n          1           1           1           1           1           1 \n   Jan 2011    Jan 2012    Jan 2013        July        June       March \n          1           1           1          10          10          10 \n        May    November     October   September \n         10          10          10          10 \n\n\n\n\n\n\n\n\nDate Format 2\n\n\n\nIn the date column, January has a year indicator, 10 of the months appear 10 times, February appears 8 times and February/1 (a leap year) appears twice. We have to delete the /1 in February (mutate) and extend the year indicator from January to the rest of the months (separate and fill).\n\n\n\n\nCode\neggs <- eggs_og %>%\n  mutate(date = str_remove(date, \" /1\")) %>%\n  separate(date, into=c(\"month\", \"year\"), sep=\" \") %>%\n  fill(year)\neggs\n\n\n# A tibble: 120 × 6\n   month     year  xlarge_dzn xlarge_halfdzn large_dzn large_halfdzn\n   <chr>     <chr>      <dbl>          <dbl>     <dbl>         <dbl>\n 1 Jan       2004        230            132       230           126 \n 2 February  2004        230            134.      226.          128.\n 3 March     2004        230            137       225           131 \n 4 April     2004        234.           137       225           131 \n 5 May       2004        236            137       225           131 \n 6 June      2004        241            137       231.          134.\n 7 July      2004        241            137       234.          134.\n 8 August    2004        241            137       234.          134.\n 9 September 2004        241            136.      234.          130.\n10 October   2004        241            136.      234.          128.\n# … with 110 more rows\n\n\n\n\n\n\n\n\nDate Format 3\n\n\n\nWe need to mutate the Month and Year into one combined date. In order to do that we would mutate and make_date time. However when I try that I get an error. I’m not entirely sure why. I tried renaming Jan to January to see if that was the issue. It still won’t let me so I will just use the mutate and str_c functions to create the date column.\n\n\n\n\nCode\neggs<-eggs %>%\n  mutate(month=recode (month, 'Jan'='January')) %>%\n  mutate(date = str_c(month, year, sep=\" \"),\n         date = my(date))\nselect(eggs, month, year, date)\n\n\n# A tibble: 120 × 3\n   month     year  date      \n   <chr>     <chr> <date>    \n 1 January   2004  2004-01-01\n 2 February  2004  2004-02-01\n 3 March     2004  2004-03-01\n 4 April     2004  2004-04-01\n 5 May       2004  2004-05-01\n 6 June      2004  2004-06-01\n 7 July      2004  2004-07-01\n 8 August    2004  2004-08-01\n 9 September 2004  2004-09-01\n10 October   2004  2004-10-01\n# … with 110 more rows\n\n\n\n\nCode\neggs<-eggs%>%\n  mutate(date = make_datetime(month, year))\n\n\nError in `mutate()`:\nℹ In argument: `date = make_datetime(month, year)`.\nCaused by error:\n! Invalid input type, expected 'integer' actual 'character'"
  },
  {
    "objectID": "posts/Sue-EllenDuffy_Challenge_5.html",
    "href": "posts/Sue-EllenDuffy_Challenge_5.html",
    "title": "AirBnB New York 2019 Challenge 5",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(readr)\nlibrary(readxl)\nlibrary(lubridate)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(ggmap)\nlibrary(maps)\nlibrary(leaflet)\nlibrary(broom)\nlibrary(summarytools)\nlibrary(hrbrthemes)\nlibrary(plotly)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/Sue-EllenDuffy_Challenge_5.html#read-in-the-data",
    "href": "posts/Sue-EllenDuffy_Challenge_5.html#read-in-the-data",
    "title": "AirBnB New York 2019 Challenge 5",
    "section": "Read in the Data",
    "text": "Read in the Data\nI chose not to pivot this data because each listing was unique, even if a host had different listings, each had different price points, neighborhoods, room types, and names.\n\n\nCode\ndata<-read.csv(\"_data/AB_NYC_2019.csv\")\ntibble(data, 10)\n\n\n# A tibble: 48,895 × 17\n      id name      host_id host_…¹ neigh…² neigh…³ latit…⁴ longi…⁵ room_…⁶ price\n   <int> <chr>       <int> <chr>   <chr>   <chr>     <dbl>   <dbl> <chr>   <int>\n 1  2539 \"Clean &…    2787 John    Brookl… Kensin…    40.6   -74.0 Privat…   149\n 2  2595 \"Skylit …    2845 Jennif… Manhat… Midtown    40.8   -74.0 Entire…   225\n 3  3647 \"THE VIL…    4632 Elisab… Manhat… Harlem     40.8   -73.9 Privat…   150\n 4  3831 \"Cozy En…    4869 LisaRo… Brookl… Clinto…    40.7   -74.0 Entire…    89\n 5  5022 \"Entire …    7192 Laura   Manhat… East H…    40.8   -73.9 Entire…    80\n 6  5099 \"Large C…    7322 Chris   Manhat… Murray…    40.7   -74.0 Entire…   200\n 7  5121 \"BlissAr…    7356 Garon   Brookl… Bedfor…    40.7   -74.0 Privat…    60\n 8  5178 \"Large F…    8967 Shunic… Manhat… Hell's…    40.8   -74.0 Privat…    79\n 9  5203 \"Cozy Cl…    7490 MaryEl… Manhat… Upper …    40.8   -74.0 Privat…    79\n10  5238 \"Cute & …    7549 Ben     Manhat… Chinat…    40.7   -74.0 Entire…   150\n# … with 48,885 more rows, 7 more variables: minimum_nights <int>,\n#   number_of_reviews <int>, last_review <chr>, reviews_per_month <dbl>,\n#   calculated_host_listings_count <int>, availability_365 <int>, `10` <dbl>,\n#   and abbreviated variable names ¹​host_name, ²​neighbourhood_group,\n#   ³​neighbourhood, ⁴​latitude, ⁵​longitude, ⁶​room_type\n\n\nThe different data point for each listing:\n\n\nCode\ncolnames(data)\n\n\n [1] \"id\"                             \"name\"                          \n [3] \"host_id\"                        \"host_name\"                     \n [5] \"neighbourhood_group\"            \"neighbourhood\"                 \n [7] \"latitude\"                       \"longitude\"                     \n [9] \"room_type\"                      \"price\"                         \n[11] \"minimum_nights\"                 \"number_of_reviews\"             \n[13] \"last_review\"                    \"reviews_per_month\"             \n[15] \"calculated_host_listings_count\" \"availability_365\""
  },
  {
    "objectID": "posts/Sue-EllenDuffy_Challenge_5.html#univariate-graphs",
    "href": "posts/Sue-EllenDuffy_Challenge_5.html#univariate-graphs",
    "title": "AirBnB New York 2019 Challenge 5",
    "section": "Univariate Graphs:",
    "text": "Univariate Graphs:"
  },
  {
    "objectID": "posts/Sue-EllenDuffy_Challenge_5.html#bivariate-graph",
    "href": "posts/Sue-EllenDuffy_Challenge_5.html#bivariate-graph",
    "title": "AirBnB New York 2019 Challenge 5",
    "section": "Bivariate Graph:",
    "text": "Bivariate Graph:"
  },
  {
    "objectID": "posts/Sue-EllenDuffy_Challenge_5.html#maps---a-trial-and-error",
    "href": "posts/Sue-EllenDuffy_Challenge_5.html#maps---a-trial-and-error",
    "title": "AirBnB New York 2019 Challenge 5",
    "section": "Maps - A trial and error",
    "text": "Maps - A trial and error\nFor Fun! - anyone want to add onto it? I’d be curious to see what others could come up with."
  },
  {
    "objectID": "posts/Sue-EllenDuffy_Challenge_6.html",
    "href": "posts/Sue-EllenDuffy_Challenge_6.html",
    "title": "Challenge 6 Air BnB Data by Last Review",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(summarytools)\nlibrary(lubridate)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/Sue-EllenDuffy_Challenge_6.html#airbnb-listing-data-in-new-york-city-2019",
    "href": "posts/Sue-EllenDuffy_Challenge_6.html#airbnb-listing-data-in-new-york-city-2019",
    "title": "Challenge 6 Air BnB Data by Last Review",
    "section": "AirBnB Listing Data in New York City 2019",
    "text": "AirBnB Listing Data in New York City 2019\nThis dataset shows AirBnB listings in NYC in 2019 with 48,895 rows (listings) and 17 columns (data for each listing). We see different types of observations including NYC neighborhood and neighborhood group, type of rental (entire home, private room, shared room), their prices, the minimum required number of nights, and number of guest reviews. Additionally we can see how many listing each host has on AirBnB, how many days a listing was available throughout 2019, and the date of the last guest review."
  },
  {
    "objectID": "posts/Sue-EllenDuffy_Challenge_6.html#read-in-the-data",
    "href": "posts/Sue-EllenDuffy_Challenge_6.html#read-in-the-data",
    "title": "Challenge 6 Air BnB Data by Last Review",
    "section": "Read in the Data",
    "text": "Read in the Data\nI chose not to pivot this data because each listing was unique, even if a host had different listings, each had different price points, neighborhoods, room types, and names.\n\n\nCode\nmydata <- read.csv(\"_data/AB_NYC_2019.csv\", na.strings=c('','  ','   '))\ntibble(mydata, 10)\n\n\n# A tibble: 48,895 × 17\n      id name      host_id host_…¹ neigh…² neigh…³ latit…⁴ longi…⁵ room_…⁶ price\n   <int> <chr>       <int> <chr>   <chr>   <chr>     <dbl>   <dbl> <chr>   <int>\n 1  2539 \"Clean &…    2787 John    Brookl… Kensin…    40.6   -74.0 Privat…   149\n 2  2595 \"Skylit …    2845 Jennif… Manhat… Midtown    40.8   -74.0 Entire…   225\n 3  3647 \"THE VIL…    4632 Elisab… Manhat… Harlem     40.8   -73.9 Privat…   150\n 4  3831 \"Cozy En…    4869 LisaRo… Brookl… Clinto…    40.7   -74.0 Entire…    89\n 5  5022 \"Entire …    7192 Laura   Manhat… East H…    40.8   -73.9 Entire…    80\n 6  5099 \"Large C…    7322 Chris   Manhat… Murray…    40.7   -74.0 Entire…   200\n 7  5121 \"BlissAr…    7356 Garon   Brookl… Bedfor…    40.7   -74.0 Privat…    60\n 8  5178 \"Large F…    8967 Shunic… Manhat… Hell's…    40.8   -74.0 Privat…    79\n 9  5203 \"Cozy Cl…    7490 MaryEl… Manhat… Upper …    40.8   -74.0 Privat…    79\n10  5238 \"Cute & …    7549 Ben     Manhat… Chinat…    40.7   -74.0 Entire…   150\n# … with 48,885 more rows, 7 more variables: minimum_nights <int>,\n#   number_of_reviews <int>, last_review <chr>, reviews_per_month <dbl>,\n#   calculated_host_listings_count <int>, availability_365 <int>, `10` <dbl>,\n#   and abbreviated variable names ¹​host_name, ²​neighbourhood_group,\n#   ³​neighbourhood, ⁴​latitude, ⁵​longitude, ⁶​room_type\n\n\nThe different data points for each listing:\n\n\nCode\ncolnames(mydata)\n\n\n [1] \"id\"                             \"name\"                          \n [3] \"host_id\"                        \"host_name\"                     \n [5] \"neighbourhood_group\"            \"neighbourhood\"                 \n [7] \"latitude\"                       \"longitude\"                     \n [9] \"room_type\"                      \"price\"                         \n[11] \"minimum_nights\"                 \"number_of_reviews\"             \n[13] \"last_review\"                    \"reviews_per_month\"             \n[15] \"calculated_host_listings_count\" \"availability_365\""
  },
  {
    "objectID": "posts/Sue-EllenDuffy_Challenge_6.html#date-tidying",
    "href": "posts/Sue-EllenDuffy_Challenge_6.html#date-tidying",
    "title": "Challenge 6 Air BnB Data by Last Review",
    "section": "Date Tidying",
    "text": "Date Tidying\nThe date was originally characters, I used transform and as.date to mutate last_review into date format.\n\n\nCode\nmydata <- transform(mydata, last_review=as.Date(last_review))\nstr(mydata)\n\n\n'data.frame':   48895 obs. of  16 variables:\n $ id                            : int  2539 2595 3647 3831 5022 5099 5121 5178 5203 5238 ...\n $ name                          : chr  \"Clean & quiet apt home by the park\" \"Skylit Midtown Castle\" \"THE VILLAGE OF HARLEM....NEW YORK !\" \"Cozy Entire Floor of Brownstone\" ...\n $ host_id                       : int  2787 2845 4632 4869 7192 7322 7356 8967 7490 7549 ...\n $ host_name                     : chr  \"John\" \"Jennifer\" \"Elisabeth\" \"LisaRoxanne\" ...\n $ neighbourhood_group           : chr  \"Brooklyn\" \"Manhattan\" \"Manhattan\" \"Brooklyn\" ...\n $ neighbourhood                 : chr  \"Kensington\" \"Midtown\" \"Harlem\" \"Clinton Hill\" ...\n $ latitude                      : num  40.6 40.8 40.8 40.7 40.8 ...\n $ longitude                     : num  -74 -74 -73.9 -74 -73.9 ...\n $ room_type                     : chr  \"Private room\" \"Entire home/apt\" \"Private room\" \"Entire home/apt\" ...\n $ price                         : int  149 225 150 89 80 200 60 79 79 150 ...\n $ minimum_nights                : int  1 1 3 1 10 3 45 2 2 1 ...\n $ number_of_reviews             : int  9 45 0 270 9 74 49 430 118 160 ...\n $ last_review                   : Date, format: \"2018-10-19\" \"2019-05-21\" ...\n $ reviews_per_month             : num  0.21 0.38 NA 4.64 0.1 0.59 0.4 3.47 0.99 1.33 ...\n $ calculated_host_listings_count: int  6 2 1 1 1 1 1 1 1 4 ...\n $ availability_365              : int  365 355 365 194 0 129 0 220 0 188 ..."
  },
  {
    "objectID": "posts/Sue-EllenDuffy_Challenge_6.html#time-dependent-visualization",
    "href": "posts/Sue-EllenDuffy_Challenge_6.html#time-dependent-visualization",
    "title": "Challenge 6 Air BnB Data by Last Review",
    "section": "Time Dependent Visualization",
    "text": "Time Dependent Visualization\nThis is the hard part. I’m not exactly sure what to show here, but I got this far so I won’t give up!\n#1 There has gotta be something wrong with these outliers. Who is staying for more than 200 days?\n\n\nCode\nggplot(mydata, aes(x=`last_review`, y=`minimum_nights`)) + geom_point() +\n  labs(title = \"Date of Last Review by Minimum Nights Stay\", x = \"Date of Last Review\", y = \"Minimum Required Nights Stay\") \n\n\n\n\n\n#2 I am just going to making some assumptions here… Those folks who had their entire home/apt for rent and had last reviews in 2012-2014 probably sold their homes. These may have been some of those early starters who were buying up real estate and saw this as a great source of income before they were able to sell and move on to other ventures.\n\n\nCode\nggplot(mydata, aes(x=`last_review`, y=`room_type`)) + geom_point() +\n    labs(title = \"Date of Last Review by Room Type\", x = \"Date of Last Review\", y = \"Room Type\") \n\n\n\n\n\n#3 I filtered here for Staten Island, just for fun, and set the listing price to less than $500. Most of the last reviews here are in 2019.\n\n\nCode\nmydata%>%\n  filter(neighbourhood_group == \"Staten Island\", price < 500) %>%\n    ggplot(aes(x=last_review, y=price)) +\n  geom_point() + scale_x_date(date_labels = \"%Y %m %d\") +\n  xlab(\"\")  + scale_x_date(limits = as.Date(c(\"2012-01-01\", \"2020-01-01\"))) +\n    labs(title = \"Staten Island - Date of Last Review by Price of Listing\", x = \"Date of Last Review\", y = \"Price of Listings\") \n\n\n\n\n\n#4 Here’s a comparasson - Manhattan has last reviews all over the place.\n\n\nCode\nmydata%>%\n  filter(neighbourhood_group == \"Manhattan\", price < 500) %>%\n    ggplot(aes(x=last_review, y=price)) +\n  geom_point() + scale_x_date(date_labels = \"%Y %m %d\") +\n  xlab(\"\")   + scale_x_date(limits = as.Date(c(\"2012-01-01\", \"2020-01-01\"))) +\n    labs(title = \"Manhattan - Date of Last Review by Price of Listing\", x = \"Date of Last Review\", y = \"Price of Listings\") \n\n\n\n\n\n#5 I’m not exactly sure what to make of this graph. But I wanted to make it so here it is!\n\n\nCode\nmydata%>%\n  filter(last_review > \"2019-01-01\") %>%\n\nggplot(aes(x=last_review, y=price, fill=neighbourhood_group)) +\n    geom_area() + ylim(0, 500) +\n    labs(title = \"2019 Neighborhood Group Data by date of last review and price\", x = \"Date of Last Review\", y = \"Price of Listings\", fill = \"Neighborhood Group\")"
  },
  {
    "objectID": "posts/Sue-EllenDuffy_Challenge_6.html#visualizing-part-whole-relationships",
    "href": "posts/Sue-EllenDuffy_Challenge_6.html#visualizing-part-whole-relationships",
    "title": "Challenge 6 Air BnB Data by Last Review",
    "section": "Visualizing Part-Whole Relationships",
    "text": "Visualizing Part-Whole Relationships\n#1 I chose this date to show the distribution of room type across Neighborhood Groups.\n\n\nCode\nggplot(mydata, aes(neighbourhood_group, fill = room_type)) + \n  geom_bar(position=\"fill\", stat = \"count\") + \n  labs(title = \"Percent Distribution of Room Type in each NYC Neighborhood Group\", x = \"Neighborhood Group\", y = \"Percentage of Room Type\") +\n  scale_fill_discrete(name = \"Room Type\") + \n  theme_bw()"
  },
  {
    "objectID": "posts/Sue-EllenDuffy_Challenge_7.html",
    "href": "posts/Sue-EllenDuffy_Challenge_7.html",
    "title": "Challenge 7 AirBnB Data with Modified Maps",
    "section": "",
    "text": "library(tidyverse)\nlibrary(ggplot2)\nlibrary(treemap)\nlibrary(treemapify)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/Sue-EllenDuffy_Challenge_7.html#airbnb-listing-data-in-new-york-city-2019",
    "href": "posts/Sue-EllenDuffy_Challenge_7.html#airbnb-listing-data-in-new-york-city-2019",
    "title": "Challenge 7 AirBnB Data with Modified Maps",
    "section": "AirBnB Listing Data in New York City 2019",
    "text": "AirBnB Listing Data in New York City 2019\nThis dataset shows AirBnB listings in NYC in 2019 with 48,895 rows (listings) and 17 columns (data for each listing). We see different types of observations including NYC neighborhood and neighborhood group, type of rental (entire home, private room, shared room), their prices, the minimum required number of nights, and number of guest reviews. Additionally we can see how many listing each host has on AirBnB, how many days a listing was available throughout 2019, and the date of the last guest review."
  },
  {
    "objectID": "posts/Sue-EllenDuffy_Challenge_7.html#read-in-the-data",
    "href": "posts/Sue-EllenDuffy_Challenge_7.html#read-in-the-data",
    "title": "Challenge 7 AirBnB Data with Modified Maps",
    "section": "Read in the Data",
    "text": "Read in the Data\nI chose not to pivot this data because each listing was unique, even if a host had different listings, each had different price points, neighborhoods, room types, and names.\n\nmydata <- read.csv(\"_data/AB_NYC_2019.csv\", na.strings=c('','  ','   '))\ntibble(mydata, 10)\n\n# A tibble: 48,895 × 17\n      id name      host_id host_…¹ neigh…² neigh…³ latit…⁴ longi…⁵ room_…⁶ price\n   <int> <chr>       <int> <chr>   <chr>   <chr>     <dbl>   <dbl> <chr>   <int>\n 1  2539 \"Clean &…    2787 John    Brookl… Kensin…    40.6   -74.0 Privat…   149\n 2  2595 \"Skylit …    2845 Jennif… Manhat… Midtown    40.8   -74.0 Entire…   225\n 3  3647 \"THE VIL…    4632 Elisab… Manhat… Harlem     40.8   -73.9 Privat…   150\n 4  3831 \"Cozy En…    4869 LisaRo… Brookl… Clinto…    40.7   -74.0 Entire…    89\n 5  5022 \"Entire …    7192 Laura   Manhat… East H…    40.8   -73.9 Entire…    80\n 6  5099 \"Large C…    7322 Chris   Manhat… Murray…    40.7   -74.0 Entire…   200\n 7  5121 \"BlissAr…    7356 Garon   Brookl… Bedfor…    40.7   -74.0 Privat…    60\n 8  5178 \"Large F…    8967 Shunic… Manhat… Hell's…    40.8   -74.0 Privat…    79\n 9  5203 \"Cozy Cl…    7490 MaryEl… Manhat… Upper …    40.8   -74.0 Privat…    79\n10  5238 \"Cute & …    7549 Ben     Manhat… Chinat…    40.7   -74.0 Entire…   150\n# … with 48,885 more rows, 7 more variables: minimum_nights <int>,\n#   number_of_reviews <int>, last_review <chr>, reviews_per_month <dbl>,\n#   calculated_host_listings_count <int>, availability_365 <int>, `10` <dbl>,\n#   and abbreviated variable names ¹​host_name, ²​neighbourhood_group,\n#   ³​neighbourhood, ⁴​latitude, ⁵​longitude, ⁶​room_type"
  },
  {
    "objectID": "posts/Sue-EllenDuffy_Challenge_7.html#date-tidying",
    "href": "posts/Sue-EllenDuffy_Challenge_7.html#date-tidying",
    "title": "Challenge 7 AirBnB Data with Modified Maps",
    "section": "Date Tidying",
    "text": "Date Tidying\nThe date was originally characters, I used transform and as.date to mutate last_review into date format.\n\nmydata <- transform(mydata, last_review=as.Date(last_review))"
  },
  {
    "objectID": "posts/Sue-EllenDuffy_Challenge_7.html#visualization-with-multiple-dimensions",
    "href": "posts/Sue-EllenDuffy_Challenge_7.html#visualization-with-multiple-dimensions",
    "title": "Challenge 7 AirBnB Data with Modified Maps",
    "section": "Visualization with Multiple Dimensions",
    "text": "Visualization with Multiple Dimensions\nIn this series of graphs I was intentional about matching colors in neighborhood groups. I believe this will give the reader an easier time making connections between neighborhood groups.\n\nggplot(mydata, aes(longitude, latitude, color = neighbourhood_group), group = neighbourhood_group) + geom_point() +\n  labs (size = \"Price of Property\", color = \"NYC Neighborhoods\", title = \"NYC AirBnB by Neighborhood Groups\")\n\n\n\n\nThe above map gives us an overview of where the units are mapped, and below we can see that while, Brooklyn and Manhattan have similar amounts of Airbnb units, Staten island and Bronx have very few comparatively.\n\nmydata %>%\n  count(neighbourhood_group) %>%\n  ggplot(aes(area= n, fill= neighbourhood_group, label = neighbourhood_group)) + \n  geom_treemap() + \n  labs(title = \"Airbnb Units by Neighborhood Group\") + \n  scale_fill_discrete(name = \"Neighborhood Group\") +\n  geom_treemap_text(colour = \"black\",\n                    place = \"centre\")\n\n\n\n\nIn order to get a better sense of the price, I removed outliers of +$500.\n\ngg<- ggplot(mydata, aes(neighbourhood_group, price, color = neighbourhood_group)) + geom_boxplot() + ylim(0, 500) + \n  labs (x = \"Neighbourhood Group\", y = \"Price of Property\") \nplot(gg) + labs(title = \"NYC AirBnB Property Prices (Under $500) in 2019 by Neighborhood Group\")\n\n\n\n\n\n\n\nHere we can see the average price per neighborhood group and room type, giving us an understanding of how each neighborhood group prices their units. For example we can see here that a private home in Manhattan is roughly the same price as an entire home/apt in Bronx and Staten Island.\n\ngg + facet_wrap ( ~ room_type) + labs(title = \"NYC AirBnB Property Prices (Under $500) in 2019 by Neighborhood Group and Room Type\", color = \"Neighborhood Group\" )  + theme(axis.text.x = element_text(angle = 90), plot.title = element_text(size = 9.5))"
  },
  {
    "objectID": "posts/Sue-EllenDuffy_Challenge_8.html",
    "href": "posts/Sue-EllenDuffy_Challenge_8.html",
    "title": "Challenge 8",
    "section": "",
    "text": "library(tidyverse)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(readr)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)\n\n\nReading in the data\nThis dataset will look at SNL actor/actress gender over time.\n\nactors <- read_csv(\"_data/snl_actors.csv\")\ncasts <- read_csv(\"_data/snl_casts.csv\")\nseasons <- read_csv(\"_data/snl_seasons.csv\")\n\n\ntibble(actors, 10)\n\n# A tibble: 2,306 × 5\n   aid            url           type  gender   `10`\n   <chr>          <chr>         <chr> <chr>   <dbl>\n 1 Kate McKinnon  /Cast/?KaMc   cast  female     10\n 2 Alex Moffat    /Cast/?AlMo   cast  male       10\n 3 Ego Nwodim     /Cast/?EgNw   cast  unknown    10\n 4 Chris Redd     /Cast/?ChRe   cast  male       10\n 5 Kenan Thompson /Cast/?KeTh   cast  male       10\n 6 Carey Mulligan /Guests/?3677 guest andy       10\n 7 Marcus Mumford /Guests/?3679 guest male       10\n 8 Aidy Bryant    /Cast/?AiBr   cast  female     10\n 9 Steve Higgins  /Crew/?StHi   crew  male       10\n10 Mikey Day      /Cast/?MiDa   cast  male       10\n# … with 2,296 more rows\n\n\n\ntibble(casts, 10)\n\n# A tibble: 614 × 9\n   aid                sid featured first…¹ last_…² updat…³ n_epi…⁴ seaso…⁵  `10`\n   <chr>            <dbl> <lgl>      <dbl>   <dbl> <lgl>     <dbl>   <dbl> <dbl>\n 1 A. Whitney Brown    11 TRUE      1.99e7      NA FALSE         8   0.444    10\n 2 A. Whitney Brown    12 TRUE     NA           NA FALSE        20   1        10\n 3 A. Whitney Brown    13 TRUE     NA           NA FALSE        13   1        10\n 4 A. Whitney Brown    14 TRUE     NA           NA FALSE        20   1        10\n 5 A. Whitney Brown    15 TRUE     NA           NA FALSE        20   1        10\n 6 A. Whitney Brown    16 TRUE     NA           NA FALSE        20   1        10\n 7 Alan Zweibel         5 TRUE      1.98e7      NA FALSE         5   0.25     10\n 8 Sasheer Zamata      39 TRUE      2.01e7      NA FALSE        11   0.524    10\n 9 Sasheer Zamata      40 TRUE     NA           NA FALSE        21   1        10\n10 Sasheer Zamata      41 FALSE    NA           NA FALSE        21   1        10\n# … with 604 more rows, and abbreviated variable names ¹​first_epid, ²​last_epid,\n#   ³​update_anchor, ⁴​n_episodes, ⁵​season_fraction\n\n\n\ntibble(seasons, 10)\n\n# A tibble: 46 × 6\n     sid  year first_epid last_epid n_episodes  `10`\n   <dbl> <dbl>      <dbl>     <dbl>      <dbl> <dbl>\n 1     1  1975   19751011  19760731         24    10\n 2     2  1976   19760918  19770521         22    10\n 3     3  1977   19770924  19780520         20    10\n 4     4  1978   19781007  19790526         20    10\n 5     5  1979   19791013  19800524         20    10\n 6     6  1980   19801115  19810411         13    10\n 7     7  1981   19811003  19820522         20    10\n 8     8  1982   19820925  19830514         20    10\n 9     9  1983   19831008  19840512         19    10\n10    10  1984   19841006  19850413         17    10\n# … with 36 more rows\n\n\nIn order to join these datasets I will have to join actors and casts by “aid” or actor ID and then join that with seasons through “sid” or season ID.\n\nunique(actors$ \"type\")\n\n[1] \"cast\"    \"guest\"   \"crew\"    \"unknown\"\n\n\nAs a way to filter out the potential skewing of data through guests starring or crew (though those would also be interesting to look at later) I will filter these out of our datasets.\n\ncasts_gender_count <- casts %>%\n  left_join(filter(actors, type==\"cast\"), \n            by=\"aid\") %>%\n  count(sid, gender)\ntibble(casts_gender_count)\n\n# A tibble: 102 × 3\n     sid gender     n\n   <dbl> <chr>  <int>\n 1     1 female     3\n 2     1 male       6\n 3     2 female     3\n 4     2 male       5\n 5     3 female     3\n 6     3 male       6\n 7     4 female     3\n 8     4 male       6\n 9     5 female     3\n10     5 male      12\n# … with 92 more rows\n\n\n\ncast_prop_all <- casts_gender_count %>%\n  group_by(sid) %>%\n  mutate(prop=n/sum(n)) %>%\n  ungroup () %>%\n  select(-n) %>%\n  pivot_wider(names_from = gender,\n              values_from = prop) %>%\n  mutate(across(everything(),~replace_na(.,0))) %>%\n  pivot_longer(c(female, male, `NA`, unknown),\n               values_to = \"prop\",\n               names_to = \"gender\")\ntibble(cast_prop_all)\n\n# A tibble: 184 × 3\n     sid gender   prop\n   <dbl> <chr>   <dbl>\n 1     1 female  0.333\n 2     1 male    0.667\n 3     1 NA      0    \n 4     1 unknown 0    \n 5     2 female  0.375\n 6     2 male    0.625\n 7     2 NA      0    \n 8     2 unknown 0    \n 9     3 female  0.333\n10     3 male    0.667\n# … with 174 more rows\n\n\n\n\nVisualization"
  },
  {
    "objectID": "posts/Sue-EllenDuffy_Final_Project_Draft.html",
    "href": "posts/Sue-EllenDuffy_Final_Project_Draft.html",
    "title": "Final Project Assignment#2: Sue-Ellen Duffy",
    "section": "",
    "text": "library(tidyverse)\nlibrary(summarytools)\nlibrary(lubridate)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(hrbrthemes)\nlibrary(scales)\nlibrary(sf)\nlibrary(plotly)\nlibrary(timeDate)\nlibrary(forcats)\nlibrary(hrbrthemes)\nlibrary(stringr)\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/Sue-EllenDuffy_Final_Project_Draft.html#part-1.-introduction",
    "href": "posts/Sue-EllenDuffy_Final_Project_Draft.html#part-1.-introduction",
    "title": "Final Project Assignment#2: Sue-Ellen Duffy",
    "section": "Part 1. Introduction",
    "text": "Part 1. Introduction\n\nData Introduction\n\nThe dataset was retrieved from the Massachusetts Department of Transportation data extraction service. I requested publicly available crash data for Boston municipality for the specific years of 2012-2022 and received a CSV file.\nThis dataset includes 45,980 individual crashes in Boston from 2012-2022 which are represented by each row in the dataset. The dataset includes crash information including date, time, weather and lighting conditions and the severity of each crash. In regards to location, the dataset includes the travel direction of the vehicles, proximity to certain landmarks such as an exit or roadway intersection and the coordinates (latitude and longitude) of the crash point. There are insert number of different data points per crash with some NAs or Unreported or Missing information.\nPost-pandemic car ownership and commuting by car has increased in an astonishing way - not to help the matter, train commutes are slower than ever. This dataset cannot delve deep enough to understand the scope of this transit issue and traffic data is not publicly available. This dataset will at least allow me to analyze city crashes that may yield some understanding about the implications of an increase in car commuting to the city. Insert some data from Inrix Potentially mention Vision Zero Boston\nQuestions\n\nAre crashes in Boston increasing overtime? What type if any are increasing over time?\nAre there any correlations to time of day, date, or weather conditions that implicate a higher severity of crash and or a higher prevalence/count of crash.\nHow did the pandemic affect Boston crashes?"
  },
  {
    "objectID": "posts/Sue-EllenDuffy_Final_Project_Draft.html#part-2.-describe-the-data-set",
    "href": "posts/Sue-EllenDuffy_Final_Project_Draft.html#part-2.-describe-the-data-set",
    "title": "Final Project Assignment#2: Sue-Ellen Duffy",
    "section": "Part 2. Describe the data set",
    "text": "Part 2. Describe the data set\n\nmydataog <- read_csv(\"Sue-EllenDuffy_FinalProjectData/Crash_Details_2012-2022.csv\", \n    skip = 2)\nmydata <- mydataog\n\n\nmydata$Crash_Date <- as.Date(mydata$Crash_Date, \"%d-%b-%Y\")\nmydata$Weekday <-wday(mydata$Crash_Date, label = TRUE, abbr = FALSE)\nmydata$Month <- month(mydata$Crash_Date, label = TRUE, abbr = TRUE)\nmydata$Year <- year(mydata$Crash_Date)\n\n\nmydata<-mydata %>%\n  mutate(Crash_Severity = recode(Crash_Severity, `Property damage only (none injured)` = \"Property damage\",\n `Unknown` = \"Unknown/Not Reported\", `Not Reported` = \"Unknown/Not Reported\"))\n\n\nmydata<-mydata%>%\n  mutate(Crash_Hour=hour(Crash_Time))\nmydata <- mydata %>%\n    mutate(Crash_Timegroup = case_when(Crash_Hour>=6 & Crash_Hour<=9 ~ \"Morning\", Crash_Hour>=10 & Crash_Hour<= 13 ~ \"Midday\", Crash_Hour>=14 & Crash_Hour<=17 ~ \"Afternoon\", Crash_Hour>=18 & Crash_Hour<=21 ~ \"Evening\", Crash_Hour>= 22 & Crash_Hour >=23 ~ \"Late Night\", Crash_Hour>= 1 & Crash_Hour >=0 ~ \"Late Night\", Crash_Hour== \"0\" ~ \"Late Night\",Crash_Hour>=2 & Crash_Hour<=5 ~ \"Overnight\"))\n\n\nmydata<- mydata %>%\n   group_by(Crash_Date) %>%\n   mutate(Crash_Count = n())\n\n\nmydata<-mydata%>%\n    mutate(Crash_Countgroup = \n             case_when(Crash_Count>=1 & Crash_Count<= 5 ~ \"1-5\", \n                       Crash_Count>=6 & Crash_Count<=10 ~ \"6-10\", \n                       Crash_Count>=11 & Crash_Count<=15 ~ \"11-15\",  \n                       Crash_Count>= 16 & Crash_Count<=20 ~ \"16-20\",\n                       Crash_Count>=21 & Crash_Count<=30 ~ \"21-30\",\n                       Crash_Count>=31 & Crash_Count<=50 ~ \"31-37\"))\n\n\ndim(mydata)\n\n[1] 45980    32\n\nhead(mydata)\n\n\n\n  \n\n\n\n\nColumns/Data Description - from original data\n\nCrash_Date - Date occurrence of crash (year, month, and day)\nCrash_Time - Time occurrence of crash (hour, min, and sec)\nCrash_Severity - Indicates the severity of a crash based on the most severe injury to any person based on 3 levels - Fatal injury,\nNon-fatal injury, Property damage only (no injury) - and either Unknown or Not Reported\nMaximum_Injury_Severity_Reported - Reported injury if both fatal and non-fatal will be categorized “Fatal injury” as it is the most severe reported injury\nNumber_of_Vehicles - Number of vehicles involved in crash occurrence\nTotal_Nonfatal_Injuries -\nTotal_Fatal_Injuries -\nWeather_Condition - The prevailing and secondary (if applicable) atmospheric conditions at the time of crash\nNon_Motorist_Type - The type of non-motorist\n\n\n\nColumns/Data Description - added for analysis\n\nCrash_Hour - Time occurrence of crash (hour only)\nCrash_Timegroup - Time occurrence of crash (time intervals - fill this in)\nCrash_Count - Number of crash occurrences per day\nCrash_Countgroup - Number of crash occurrences per day (defined by group intervals - fill this in)\n\n\n\nDate and Time Tidying\nSanity Check - Any Crashes reported more than once? No. Duplicate was run and the row counts remained the same. > mydata[!duplicated(mydata$Crash_Number), ] # A tibble: 45,980 × 28 >"
  },
  {
    "objectID": "posts/Sue-EllenDuffy_Final_Project_Draft.html#crash-data-by-severity",
    "href": "posts/Sue-EllenDuffy_Final_Project_Draft.html#crash-data-by-severity",
    "title": "Final Project Assignment#2: Sue-Ellen Duffy",
    "section": "Crash Data by Severity",
    "text": "Crash Data by Severity\nThere were 45,980 reported from 2012 to 2022. Of those crashes the majority are reported as resulting in property damage only with no injuries reported. Ranking second is non-fatal injuries, and finally a small portion are rated as fatal injuries. 4,002 crashes, while reported to MassDot are missing report data on crash severity and are reported as “Unknown” or “Not Reported. For the purpose of this report, I have combined this category into”Unknown/Not Reported”. These “Unknown/Not Reported” crashes make up 8.7% of the crash data but we do not know the crash severity and therefore cannot use this data during certain analysis. In the following data I will note on each graph if I have removed these missing data from analysis.\n\ntabseverities <- table(mydata$City_Town_Name, mydata$Crash_Severity)\ntabseverities\n\n        \n         Fatal injury Non-fatal injury Property damage Unknown/Not Reported\n  BOSTON          248            12804           28926                 4002\n\nggplot(mydata, aes(x=Crash_Severity, fill = Crash_Severity)) + geom_histogram(stat = \"count\")"
  },
  {
    "objectID": "posts/Sue-EllenDuffy_Final_Project_Draft.html#crash-data-by-datetime",
    "href": "posts/Sue-EllenDuffy_Final_Project_Draft.html#crash-data-by-datetime",
    "title": "Final Project Assignment#2: Sue-Ellen Duffy",
    "section": "Crash Data by Date/Time",
    "text": "Crash Data by Date/Time\n\nOverall Data\n\nallplot<- ggplot(mydata, aes(Crash_Date, stat=\"count\")) + geom_bar()\nggplotly(allplot)\n\n\n\n\nggplot(mydata, aes(Crash_Date, y=Crash_Count, group = Crash_Severity)) + geom_point()\n\n\n\n\n\n\nCrash Data by Day\nBoston Averages 13 crashes per day (from 2012-2022).\n\n#Crash Count per Day\nmydatacountsall <- mydata %>% \n  group_by(City_Town_Name) %>% \n  summarize(\"min\" = min(Crash_Count, na.rm = TRUE),\n            \"max\" = max(Crash_Count, na.rm = TRUE),\n            \"mean\" = mean(Crash_Count, na.rm = TRUE), \n            \"median\" = median(Crash_Count, na.rm = TRUE),\n            \"standard_deviation\" = sd(Crash_Count, na.rm = TRUE)) %>%\n  arrange(City_Town_Name)\nprint(mydatacountsall)\n\n# A tibble: 1 × 6\n  City_Town_Name   min   max  mean median standard_deviation\n  <chr>          <int> <int> <dbl>  <dbl>              <dbl>\n1 BOSTON             1    37  13.2     13               4.68\n\nposition_Countgroup <- c(\"1-5\", \"6-10\", \"11-15\", \"16-20\", \"21-30\", \"31-37\")\nggplot(mydata, aes(Crash_Countgroup, stat=\"count\")) + geom_bar() + scale_x_discrete(limit =position_Countgroup)\n\n\n\n\n\n\nCrash Data by Day of the Week\nOverall, Boston sees the most crashes on Friday.\n\nmydata %>%\nggplot(aes(Weekday, fill = Weekday)) +\n  geom_bar( stat = \"count\") + theme(axis.text.x = element_text(angle = 90)) + theme(legend.position = \"none\")\n\n\n\n\nWhen subsetted for Crash Severity, however Boston sees the most fatal accidents on Wednesday and Saturday.\n\nmydata %>%\n  filter(Crash_Severity != \"Unknown/Not Reported\") %>%\nggplot(aes(Weekday, fill = Weekday)) +\n  geom_bar( stat = \"count\") + facet_wrap ( ~ Crash_Severity, scales = \"free_y\") + theme(axis.text.x = element_text(angle = 90)) + theme(legend.position = \"none\")\n\n\n\n\n\n\nCrash Data by Month\nOverall, October is the month with the highest Crash Ratings over 2012-2022. Let’s look at how each crash severity rating factors into overall highest crash ratings.\nWhat could we learn about October? Are there significant events that happen in October? Is it all happening around Halloween? Or does weather condition have something to do with the increase in accidents?\n\n#Crash Count per Day by Month\nmydatamonthsum <- mydata %>% \n  group_by(Month) %>% \n  summarize(\"min\" = min(Crash_Count, na.rm = TRUE),\n            \"max\" = max(Crash_Count, na.rm = TRUE),\n            \"mean\" = mean(Crash_Count, na.rm = TRUE), \n            \"median\" = median(Crash_Count, na.rm = TRUE),\n            \"standard_deviation\" = sd(Crash_Count, na.rm = TRUE))%>% \n  arrange(Month)\nmydatamonthsum %>%\nprint(n=12)\n\n# A tibble: 12 × 6\n   Month   min   max  mean median standard_deviation\n   <ord> <int> <int> <dbl>  <dbl>              <dbl>\n 1 Jan       1    26  12.7     12               4.56\n 2 Feb       1    37  12.6     12               4.74\n 3 Mar       1    27  12.4     12               4.48\n 4 Apr       1    26  12.2     12               4.17\n 5 May       1    28  13.2     13               4.50\n 6 Jun       1    26  13.5     13               4.35\n 7 Jul       1    32  13.2     13               4.60\n 8 Aug       3    22  12.5     12               3.64\n 9 Sep       2    28  13.4     13               4.27\n10 Oct       2    34  15.1     14               5.74\n11 Nov       1    31  13.9     13               5.40\n12 Dec       1    25  13.3     13               4.38\n\n\n\nggplot(mydata, aes(Month, stat=\"count\")) + geom_bar() \n\n\n\n\nWe can now see that the month with the highest Fatal injury is reported in July, with dips in spring, winter, and October while Non-fatal injuries and Property damage peak in October. The peak for Unknown/Not Reported crashes is in June and July.\n\nggplot(mydata, aes(Month, stat=\"count\", fill = Crash_Severity)) + geom_bar() + facet_wrap( ~ Crash_Severity, scales = \"free_y\") + labs(title = \"Crash Severity by Month (with free Y axis)\") + theme(legend.position = \"none\")\n\n\n\n\n\n\nCrash Data by Year\nOverall we see that 2017 was the year with the most Crashes from 2012-2022. The data shows 2017 as a peak with a sharp drop off in 2020 (due to the pandemic and reduced traffic flow) and a climb back up in 2021 almost matching crash reports from 2019. Of course this data shows that post-pandemic rates of car crashes has gone back up, but it is not as high as crashes in 2017. What happened in 2017 and were these crashes somehow different from crashes in 2016?\n\n#Crash Count per Day by Year\nmydatasum <- mydata %>% \n  group_by(Year) %>% \n  summarize(\"min\" = min(Crash_Count, na.rm = TRUE),\n            \"max\" = max(Crash_Count, na.rm = TRUE),\n            \"mean\" = mean(Crash_Count, na.rm = TRUE), \n            \"median\" = median(Crash_Count, na.rm = TRUE),\n            \"standard_deviation\" = sd(Crash_Count, na.rm = TRUE)) %>% \n  arrange(Year)\n\nmydatasum %>%\nprint(n=11)\n\n# A tibble: 11 × 6\n    Year   min   max  mean median standard_deviation\n   <dbl> <int> <int> <dbl>  <dbl>              <dbl>\n 1  2012     1    26  12.5     12               4.41\n 2  2013     1    31  12.4     12               4.61\n 3  2014     2    29  12.8     12               4.78\n 4  2015     1    32  13.2     13               4.59\n 5  2016     1    34  13.2     13               4.44\n 6  2017     6    37  15.8     15               4.72\n 7  2018     1    28  14.6     14               4.44\n 8  2019     4    24  13.3     13               3.92\n 9  2020     1    29  10.6     10               4.45\n10  2021     1    28  13.1     13               4.70\n11  2022     2    26  12.0     12               4.37\n\n\n\nggplot(mydata, aes(year(Crash_Date))) + geom_histogram(binwidth = .50) + scale_x_continuous(breaks=pretty_breaks())\n\n\n\n\nWhen yearly crash data is subset to look at Crash Severity we see different trends across the years. Fatal injury peaked in 2021 with 2016 close behind. Non-fatal injury peaked in 2017. Unknown/Not Reported crashes peaked in 2012 and in 2017. I wonder if in 2012 there was not an established standard to filling out reports and if in 2017 there were just more reports than could be fully processed.\n\nggplot(mydata, aes(Year, stat=\"count\", fill = Crash_Severity)) + geom_bar() + scale_x_continuous(breaks=pretty_breaks()) +facet_wrap( ~ Crash_Severity, scales = \"free_y\" ) + labs(title = \"Crash Severity by Year (with free Y axis)\") + theme(legend.position = \"none\")\n\n\n\n\n\n\nCrash Data by Time of Day\n\npositions <- c(\"Morning\", \"Midday\", \"Afternoon\", \"Evening\", \"Late Night\")\n\nggplot(mydata, aes(Crash_Time)) + geom_histogram()\n\n\n\nggplot(mydata, aes(Crash_Time, stat=\"count\", fill = Crash_Severity)) + geom_bar()+ theme(legend.position = \"bottom\")\n\n\n\nggplot(mydata, aes(Crash_Time, stat=\"count\", fill = Crash_Severity)) + geom_bar()+ theme(legend.position = \"bottom\") + facet_wrap ( ~ Crash_Severity, scales = \"free_y\") \n\n\n\nggplot(mydata, aes(Crash_Timegroup, stat=\"count\", fill = Crash_Severity)) + geom_bar() + scale_x_discrete(limits = positions)\n\n\n\nggplot(mydata, aes(Crash_Timegroup, stat=\"count\", fill = Crash_Severity)) + geom_bar() + facet_wrap ( ~ Crash_Severity, scales = \"free_y\")  + scale_x_discrete(limits = positions) + theme(legend.position = \"none\")"
  },
  {
    "objectID": "posts/Sue-EllenDuffy_Final_Project_Draft.html#crash-data-by-manner-of-collision",
    "href": "posts/Sue-EllenDuffy_Final_Project_Draft.html#crash-data-by-manner-of-collision",
    "title": "Final Project Assignment#2: Sue-Ellen Duffy",
    "section": "Crash Data by Manner of Collision",
    "text": "Crash Data by Manner of Collision\nTo no one’s surprise, we see that Rear-ends are the manner of collision that occurs most often in Boston. Following in second, third, and fourth respectively are: Single vehicle crashes,Angle crashes, and Sideswipe, same direction crashes.\n\npositions2 <- c(\"Rear-end\", \"Single vehicle crash\", \"Angle\", \"Sideswipe, same direction\", \"Not reported\", \"Head-on\", \"Sideswipe, opposite direction\", \"Unknown\", \"Front to Rear\", \"Front to Front\", \"Rear-to-rear\", \"Rear to Side\", \"NA\")\n\nmydata %>%\n  ggplot(., aes(x= Manner_of_Collision, stat = \"count\", fill = Manner_of_Collision)) + \n  geom_bar() +  labs(title = \"Manner of Collision from 2012-2022\") + coord_flip() + theme(legend.position = \"none\") + scale_x_discrete(limits = positions2) \n\n\n\nmydata %>%\n  ggplot(aes(x= Manner_of_Collision, stat = \"count\", fill = Manner_of_Collision)) + \n  geom_bar() +  labs(title = \"Manner of Collision from 2012-2022, with free-x axis\") + coord_flip() + theme(legend.position = \"none\") + facet_wrap ( ~ Crash_Severity, scales = \"free_x\") + scale_x_discrete(limits = positions2)\n\n\n\nmydata %>%\n  filter(Crash_Severity == \"Fatal injury\") %>%\n  ggplot(aes(x= Manner_of_Collision, stat = \"count\", fill = Manner_of_Collision)) + \n  geom_bar() +  labs(title = \"Manner of Collision for Fatal Crashes from 2012-2022\") + coord_flip() + theme(legend.position = \"none\")+ scale_x_discrete(limits = positions2)"
  },
  {
    "objectID": "posts/Sue-EllenDuffy_Final_Project_Draft.html#unknown-and-not-reported-incidents",
    "href": "posts/Sue-EllenDuffy_Final_Project_Draft.html#unknown-and-not-reported-incidents",
    "title": "Final Project Assignment#2: Sue-Ellen Duffy",
    "section": "Unknown and Not Reported incidents",
    "text": "Unknown and Not Reported incidents\nAll 46 reporting inconsistencies occurred between May 2018 and October 2019:\n\nCrash Severity “Non-fatal injury” has 7 reports of “no injury” and just under 300 counts of “possible” or “suspected” injuries.\nCrash Severity “Not Reported” has 42 reports of maximum injury reports across 6 categories that suggest a possible injury.\nCrash Severity “Property damage” has 11 reports of a maximum injury reported as being “not reported”.\nCrash Severity “Unknown” has 7 reports of non-fatal injury as either “Non-incapacitating” or “Possible”.\n\n\nmydataog$Crash_Date <- as.Date(mydataog$Crash_Date, \"%d-%b-%Y\")\nmydataog$Year <- year(mydataog$Crash_Date)\n\nmydataog <- mydataog %>%\nmutate(Crash_Severity = recode(Crash_Severity, `Property damage only (none injured)` = \"Property damage\"))\nmydatainconsistencies <- mydataog %>% \n  filter(between(Crash_Date, as.Date('2018-05-01'), as.Date('2019-10-31'))) \n\ntabinconsistencies <- table(mydatainconsistencies$Maximum_Injury_Severity_Reported, mydatainconsistencies$Crash_Severity) # Table for 2018-05-01:2019-10-31\ntabinconsistencies\n\n                                       \n                                        Fatal injury Non-fatal injury\n  Deceased not caused by crash                     0                3\n  Fatal injury (K)                                30                0\n  No Apparent Injury (O)                           0                0\n  No injury                                        0                1\n  Non-fatal injury - Incapacitating                0              142\n  Non-fatal injury - Non-incapacitating            0              686\n  Non-fatal injury - Possible                      0              455\n  Not Applicable                                   0                0\n  Not reported                                     0                0\n  Possible Injury (C)                              0              238\n  Suspected Minor Injury (B)                       0              273\n  Suspected Serious Injury (A)                     0               18\n  Unknown                                          0                0\n                                       \n                                        Not Reported Property damage Unknown\n  Deceased not caused by crash                     1               0       0\n  Fatal injury (K)                                 0               0       0\n  No Apparent Injury (O)                          20            1786       1\n  No injury                                       31            2766       3\n  Non-fatal injury - Incapacitating                3               0       0\n  Non-fatal injury - Non-incapacitating           14               0       3\n  Non-fatal injury - Possible                     17               0       4\n  Not Applicable                                  56               0       2\n  Not reported                                   326               0     101\n  Possible Injury (C)                              3               0       0\n  Suspected Minor Injury (B)                       3               0       0\n  Suspected Serious Injury (A)                     2               0       0\n  Unknown                                          0               0      21\n\n\n\nggplot(mydataog, aes(x=Year, fill = Crash_Severity)) + geom_bar() + facet_wrap( ~ Crash_Severity, scales = \"free_y\") + theme(legend.position = \"none\") + scale_x_continuous(breaks=pretty_breaks()) \n\n\n\nggplot(mydata, aes(x=Year, fill = Crash_Severity)) + geom_bar() + facet_wrap( ~ Crash_Severity, scales = \"free_y\") + theme(legend.position = \"none\") + scale_x_continuous(breaks=pretty_breaks()) \n\n\n\n\n\nmydata %>%\n  filter(Crash_Severity == \"Unknown/Not Reported\") %>%\n  filter(between(Crash_Date, as.Date('2018-05-01'), as.Date('2019-10-31'))) %>%\nggplot(., aes(x=Maximum_Injury_Severity_Reported, stat=\"count\", fill = Maximum_Injury_Severity_Reported)) + geom_bar() + coord_flip() + facet_wrap (~ Crash_Severity, scales = \"free_x\") + theme(legend.position = \"none\") +labs(title = str_wrap(\"Inconsistencies in Reported Crash Severity Between May 1st, 2018 and October 31st, 2019\", width = 50))"
  },
  {
    "objectID": "posts/Sue-EllenDuffy_Final_Project_Draft.html#missing-datanas-and-outliers-and-why-do-you-choose-this-way-to-deal-with-nas",
    "href": "posts/Sue-EllenDuffy_Final_Project_Draft.html#missing-datanas-and-outliers-and-why-do-you-choose-this-way-to-deal-with-nas",
    "title": "Final Project Assignment#2: Sue-Ellen Duffy",
    "section": "Missing data/NAs and outliers? And why do you choose this way to deal with NAs?",
    "text": "Missing data/NAs and outliers? And why do you choose this way to deal with NAs?\n*should I fix the 20 data that report unknown or not reported for crash severity and then list either “Non-fatal injury - Incapacitating” or “Non-fatal injury - Non-incapacitating” in Maximum_Injury_Severity_Reported?*\n\nsapply(mydata, function(x) sum(is.na(x)))\n\n                              Crash_Number \n                                         0 \n                            City_Town_Name \n                                         0 \n                                Crash_Date \n                                         0 \n                                Crash_Time \n                                         0 \n                            Crash_Severity \n                                         0 \n          Maximum_Injury_Severity_Reported \n                                         0 \n                        Number_of_Vehicles \n                                         0 \n                   Total_Nonfatal_Injuries \n                                         0 \n                      Total_Fatal_Injuries \n                                         0 \n                       Manner_of_Collision \n                                         5 \n             Vehicle_Action_Prior_to_Crash \n                                         8 \n                 Vehicle_Travel_Directions \n                                         4 \n                       Most_Harmful_Events \n                                      2639 \n                     Vehicle_Configuration \n                                       213 \n                    Road_Surface_Condition \n                                       368 \n                             Ambient_Light \n                                         1 \n                         Weather_Condition \n                                         0 \n                   At_Roadway_Intersection \n                                     33040 \nDistance_From_Nearest_Roadway_Intersection \n                                       309 \n          Distance_From_Nearest_Milemarker \n                                     43298 \n                Distance_From_Nearest_Exit \n                                     36521 \n            Distance_From_Nearest_Landmark \n                                     35704 \n                         Non_Motorist_Type \n                                     43462 \n                             X_Cooordinate \n                                      1934 \n                             Y_Cooordinate \n                                      1934 \n                                   Weekday \n                                         0 \n                                     Month \n                                         0 \n                                      Year \n                                         0 \n                                Crash_Hour \n                                         0 \n                           Crash_Timegroup \n                                         0 \n                               Crash_Count \n                                         0 \n                          Crash_Countgroup \n                                         0"
  },
  {
    "objectID": "posts/Sue-EllenDuffy_Final_Project_Proposal.html",
    "href": "posts/Sue-EllenDuffy_Final_Project_Proposal.html",
    "title": "Final Project Assignment#1: Sue-Ellen Duffy",
    "section": "",
    "text": "library(tidyverse)\nlibrary(summarytools)\nlibrary(lubridate)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(treemap)\nlibrary(hrbrthemes)\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/Sue-EllenDuffy_Final_Project_Proposal.html#describe-the-data-sets",
    "href": "posts/Sue-EllenDuffy_Final_Project_Proposal.html#describe-the-data-sets",
    "title": "Final Project Assignment#1: Sue-Ellen Duffy",
    "section": "Part 1. Introduction",
    "text": "Part 1. Introduction\n\nData Introduction\n\nThe dataset was retrieved from the Massachusetts Department of Transportation data extraction service. I requested publicly available crash data for Boston municipality for the specific years of 2012-2022 and received a CSV file.\nThis dataset is about individual crashes in Boston from 2012-2022 and are represented by each row in the dataset. The dataset includes crash information including date, time, weather and lighting conditions and the severity of each crash. In regards to location, the dataset includes the travel direction of the vehicles, proximity to certain landmarks such as an exit or roadway intersection and the coordinates (latitude and longitude) of the crash point.\nPost-pandemic car ownership and commuting by car has increased in an astonishing way - not to help the matter, train commutes are slower than ever. This dataset cannot delve deep enough to understand the scope of this issue and traffic data is not publicly available. This dataset will at least allow me to analyze city crashes that may yield some understanding about the implications of an increase in car commuting to the city.\n\nQuestions\n\nI want to know if there are any correlations to time of day, date, or weather conditions that implicate a higher severity of crash and or a higher prevalence of crash.\nI also want to know if these dates or times of day that are associated with higher prevalence of crash have any relationship with holidays, weekends, or sunset/sunrises.\nAn additional question is whether there are certain areas that contain higher crash points in the city. Do crashes happen more so on the highways between popular destinations or do they for instance happen on city streets where many drivers are searching for places to park.\nAs a final question - post-pandemic are incidents of crashes going up in the city?"
  },
  {
    "objectID": "posts/Sue-EllenDuffy_Final_Project_Proposal.html#describe-the-data-sets-1",
    "href": "posts/Sue-EllenDuffy_Final_Project_Proposal.html#describe-the-data-sets-1",
    "title": "Final Project Assignment#1: Sue-Ellen Duffy",
    "section": "Part 2. Describe the data set(s)",
    "text": "Part 2. Describe the data set(s)\n\ncrashdata <- read_csv(\"Sue-EllenDuffy_FinalProjectData/Crash_Details_2012-2022.csv\", \n    skip = 2)\nView(crashdata)\n\n\ndim(crashdata)\n\n[1] 45980    25\n\nlength(unique(crashdata))\n\n[1] 25\n\nhead(crashdata)\n\n\n\n  \n\n\nstr(crashdata)\n\nspc_tbl_ [45,980 × 25] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ Crash_Number                              : num [1:45980] 2858373 2855641 2964895 2956331 2842066 ...\n $ City_Town_Name                            : chr [1:45980] \"BOSTON\" \"BOSTON\" \"BOSTON\" \"BOSTON\" ...\n $ Crash_Date                                : chr [1:45980] \"01-Jan-2012\" \"01-Jan-2012\" \"01-Jan-2012\" \"01-Jan-2012\" ...\n $ Crash_Time                                : 'hms' num [1:45980] 00:03:00 01:31:00 01:54:00 02:00:00 ...\n  ..- attr(*, \"units\")= chr \"secs\"\n $ Crash_Severity                            : chr [1:45980] \"Property damage only (none injured)\" \"Property damage only (none injured)\" \"Property damage only (none injured)\" \"Non-fatal injury\" ...\n $ Maximum_Injury_Severity_Reported          : chr [1:45980] \"No injury\" \"No injury\" \"No injury\" \"Non-fatal injury - Non-incapacitating\" ...\n $ Number_of_Vehicles                        : num [1:45980] 1 2 2 2 1 1 1 2 2 1 ...\n $ Total_Nonfatal_Injuries                   : num [1:45980] 0 0 0 1 0 1 0 0 0 1 ...\n $ Total_Fatal_Injuries                      : num [1:45980] 0 0 0 0 0 0 0 0 0 0 ...\n $ Manner_of_Collision                       : chr [1:45980] \"Single vehicle crash\" \"Sideswipe, same direction\" \"Rear-end\" \"Angle\" ...\n $ Vehicle_Action_Prior_to_Crash             : chr [1:45980] \"V1: Travelling straight ahead\" \"V1: Travelling straight ahead / V2: Changing lanes\" \"V1: Turning right / V2: Travelling straight ahead\" \"V1: Travelling straight ahead / V2: Turning left\" ...\n $ Vehicle_Travel_Directions                 : chr [1:45980] \"V1: W\" \"V1: N  / V2: N\" \"V1: N  / V2: S\" \"V1: S  / V2: S\" ...\n $ Most_Harmful_Events                       : chr [1:45980] \"V1:(Collision with curb)\" \"V1:(Collision with motor vehicle in traffic) / V2:(Collision with motor vehicle in traffic)\" \"V1:(Collision with motor vehicle in traffic) / V2:(Unknown)\" \"V1:(Collision with motor vehicle in traffic) / V2:(Collision with motor vehicle in traffic)\" ...\n $ Vehicle_Configuration                     : chr [1:45980] \"V1:(Passenger car)\" \"V1:(Passenger car) / V2:(Passenger car)\" \"V1:(Passenger car) / V2:(Passenger car)\" \"V1:(Passenger car) / V2:(Passenger car)\" ...\n $ Road_Surface_Condition                    : chr [1:45980] \"Dry\" \"Dry\" \"Dry\" \"Dry\" ...\n $ Ambient_Light                             : chr [1:45980] \"Dark - roadway not lighted\" \"Dark - lighted roadway\" \"Daylight\" \"Dark - lighted roadway\" ...\n $ Weather_Condition                         : chr [1:45980] \"Not Reported\" \"Clear\" \"Clear/Clear\" \"Cloudy/Cloudy\" ...\n $ At_Roadway_Intersection                   : chr [1:45980] NA NA \"INTERVALE STREET / BLUE HILL AVENUE\" NA ...\n $ Distance_From_Nearest_Roadway_Intersection: chr [1:45980] \"MORTON STREET / CANTERBURY STREET\" \"LEVERETT CONNECTOR NORTH / INTERSTATE 93 Rte 93 N\" \"INTERVALE STREET / BLUE HILL AVENUE\" \"MASSACHUSETTS AVENUE / THEODORE GLYNN WAY\" ...\n $ Distance_From_Nearest_Milemarker          : chr [1:45980] NA NA NA NA ...\n $ Distance_From_Nearest_Exit                : chr [1:45980] NA NA NA NA ...\n $ Distance_From_Nearest_Landmark            : chr [1:45980] NA NA NA NA ...\n $ Non_Motorist_Type                         : chr [1:45980] NA NA NA NA ...\n $ X_Cooordinate                             : num [1:45980] 233302 234816 234564 235439 NA ...\n $ Y_Cooordinate                             : num [1:45980] 893678 903323 895797 897873 NA ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   Crash_Number = col_double(),\n  ..   City_Town_Name = col_character(),\n  ..   Crash_Date = col_character(),\n  ..   Crash_Time = col_time(format = \"\"),\n  ..   Crash_Severity = col_character(),\n  ..   Maximum_Injury_Severity_Reported = col_character(),\n  ..   Number_of_Vehicles = col_double(),\n  ..   Total_Nonfatal_Injuries = col_double(),\n  ..   Total_Fatal_Injuries = col_double(),\n  ..   Manner_of_Collision = col_character(),\n  ..   Vehicle_Action_Prior_to_Crash = col_character(),\n  ..   Vehicle_Travel_Directions = col_character(),\n  ..   Most_Harmful_Events = col_character(),\n  ..   Vehicle_Configuration = col_character(),\n  ..   Road_Surface_Condition = col_character(),\n  ..   Ambient_Light = col_character(),\n  ..   Weather_Condition = col_character(),\n  ..   At_Roadway_Intersection = col_character(),\n  ..   Distance_From_Nearest_Roadway_Intersection = col_character(),\n  ..   Distance_From_Nearest_Milemarker = col_character(),\n  ..   Distance_From_Nearest_Exit = col_character(),\n  ..   Distance_From_Nearest_Landmark = col_character(),\n  ..   Non_Motorist_Type = col_character(),\n  ..   X_Cooordinate = col_double(),\n  ..   Y_Cooordinate = col_double()\n  .. )\n - attr(*, \"problems\")=<externalptr> \n\n\n\nconduct summary statistics of the dataset(s); especially show the basic statistics (min, max, mean, median, etc.) for the variables you are interested in.\n\nVariables I am interested in:\nCrash_Time\nCrash_Date - (specifically the years, holidays, and daylight savings time, etc.)\nCrash_Severity\nManner_of_Crash\nMaximum_Injury_Severity_Reported\nVehicle_Travel_Direction\nAmbient_Light\nWeather_Condition\nX_Coordinate & Y_Coordinate\n\nunique(crashdata$\"Crash_Severity\")\n\n[1] \"Property damage only (none injured)\" \"Non-fatal injury\"                   \n[3] \"Unknown\"                             \"Not Reported\"                       \n[5] \"Fatal injury\"                       \n\nunique(crashdata$\"Manner_of_Collision\")\n\n [1] \"Single vehicle crash\"          \"Sideswipe, same direction\"    \n [3] \"Rear-end\"                      \"Angle\"                        \n [5] \"Not reported\"                  \"Unknown\"                      \n [7] \"Sideswipe, opposite direction\" \"Head-on\"                      \n [9] \"Rear-to-rear\"                  NA                             \n[11] \"Rear to Side\"                  \"Front to Rear\"                \n[13] \"Front to Front\"               \n\nunique(crashdata$\"Ambient_Light\")\n\n [1] \"Dark - roadway not lighted\"      \"Dark - lighted roadway\"         \n [3] \"Daylight\"                        \"Not reported\"                   \n [5] \"Dawn\"                            \"Dark - unknown roadway lighting\"\n [7] \"Dusk\"                            \"Other\"                          \n [9] \"Unknown\"                         NA                               \n\n\n\np<- ggplot(crashdata, aes(x=Crash_Severity)) +\n         geom_histogram(stat = \"count\")\np\n\n\n\nggplot(crashdata, aes(Manner_of_Collision, fill = Crash_Severity)) +\n  geom_bar(position = \"fill\", stat = \"count\")+ \n  coord_flip()\n\n\n\nggplot(crashdata, aes(Ambient_Light, fill = Crash_Severity)) +\n  geom_bar(position = \"fill\", stat = \"count\") + \n  coord_flip()\n\n\n\n\n\nggplot(crashdata, aes(Crash_Time)) + geom_histogram()"
  },
  {
    "objectID": "posts/Sue-EllenDuffy_Final_Project_Proposal.html#the-tentative-plan-for-visualization",
    "href": "posts/Sue-EllenDuffy_Final_Project_Proposal.html#the-tentative-plan-for-visualization",
    "title": "Final Project Assignment#1: Sue-Ellen Duffy",
    "section": "3. The Tentative Plan for Visualization",
    "text": "3. The Tentative Plan for Visualization\nThere are a lot of unknowns when it comes to crashes that resulted in fatalities. I will not be able to simply analyze the conditions of the most dangerous incidents, I will also have to analyze the other crashes which cause harm - including non-fatal accidents and property damage only.\n#Visualizations\nOverall/Summary:\nTreemap - Crash_Severity counts - to show the distribution in types of crash severity (could also be a piechart or barchart)\nCircular Packing or Treemap - to show certain groupings that stand out (for example: “Morning” - rear-ends; “Afternoon” - sideswipe, same direction; Evening” - Head-on)\nDate/Time:\nLine plot, StreamGraph - Crash_Time/Hour/Date/Year Time Series - Crash_Severity in relation to Sunset/Sunrise\n(maybe - Parallel Coordinate Plot - to show a lot of data over time and separate by groups)\nI will need to add or import a dataset for “Holidays” and “Sunset/Sunrise” and analyze crash severity and frequency by these times.\nLocation:\nBubble Map - Coordinates of crashes - with fill being counts of crash severity (fatal, non-fatal, damage to property, none).\nChoropleth Map - Coordinates & Severity of Crash_Severity or Fatality Bar chart or Violin chart of types of types of vehicle travel direction prior to crash.\nBar chart showing the proximity to landmark/exit."
  }
]